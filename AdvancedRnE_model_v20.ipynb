{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version Info\n",
    "v5 version에서 cycleGAN 부분을 아래 notebook을 바탕으로 수정 중 == v6 version<br>https://colab.research.google.com/drive/1n5OzIaic1Ho60UAtqEgCnRyn7Lg1Po_v<br>sequence 길이를 모두 1로 맞춰야 함, 그렇지 않으면 생성할 때 마다 차원이 감소함. 아니면 train 과정에서 모델이 seq 길이 만큼 하나의 데이터에 대해 반복을 수행해서 생성을 진행해야 한다.\n",
    "\n",
    "v6 version lstm cell return_sequnces=True로 설정하고 마지막 output layer에서 2차원 dense를 수행<br> --> seq_l을 데이터의 차원으로 갖도록 함\n",
    "\n",
    "cycleGAN의 generation 부분 -> v7\n",
    "\n",
    "cycleGAN의 loss 및 훈련 -> v8\n",
    "\n",
    "gradients tape 문제 해결을 위한 tf dataset 이용 -> v9\n",
    "\n",
    "v9의 cycleGAN pt type code 정리 및 파이프라인 보수 -> v10<br>\n",
    "v10에서 seq화 과정에서 발생하는 데이터 손실, loss 그래프, gpu 사용량 최적화, notes to MIDI(생성 데이터 MIDI로 변환) 수정 -> v11<br>\n",
    "v11에서 todo 6~ 까지의 수정 -> v12<br>\n",
    "v12에서 vocab size 관련 error 수정 + batch 등의 파라미터 조정 -> v13<br>\n",
    "v13에서 만든 pt type 완성본을 여러가지 방면으로 실험 -> v14<br>\n",
    "v14에서 실험+ -> v15<br>\n",
    "v15에서 seq, batch, bi, gru, tf.reduce_mean 실험 pt type(그나마 괜찮은 값) -> v16<br>\n",
    "v16에서 심알 포스터에 들어갈 자료 생성 -> v17<br>\n",
    "v17에서 발생하는 생성자의 학습 문제 및 과적합 문제 해결을 위해 판별자 lr 낮추기 -> 판별자의 학습을 늦춰 생성자의 학습 시간을 벌어주기, \n",
    "binary crossentropy 쪽에서 발생 또는 다른 loss 계산에서 발생하는 nan or inf 문제를 해결하기 위해 loss function 수정 -> v18<br>\n",
    "v18 + TimeDistributed model 적용한 거 -> v19<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "1. 일반 LSTM model과는 다르게 cycleGAN의 input data를 만들어야 한다.\n",
    "즉, sequence data로 data를 변환할 때, 일반 LSTM model과는 달리 target data를 생성하지 않아야 하며 seq_L 만큼 마다 나눠서 데이터 손실 없이 input data로 사용해야한다.\n",
    "\n",
    "    -> 모델이 seq_l 길이 만큼의 데이터를 바탕으로 (그 길이 안 데이터 해석) output을 만들어 낸다.<br> 이후 sequence data로 나뉘어진 output data를 다시 reshape한 후, midi로 변환하면 된다.\n",
    "\n",
    "2. sequence data 및 음악적 특성을 충분히 고려할 수 있도록 discriminator 및 그에 따른 loss function 구현 필요 -- 시간 계획 필수 (기한 고려)\n",
    "\n",
    "3. 향후 cycleGAN과 관련된 loss(cycle consistnecy 등)도 손을 볼 수 있으면 좋을 것 같다.\n",
    "\n",
    "4. GPU 사용량 늘리기\n",
    "\n",
    "5. batch를 만들때, shuffle 주의하기, test data에는 사용하지 않아야 할수도 있다. -> train data shuffle을 수행해도 무방할 정도의 seq_l을 잡거나 shuffle을 수행하지 않아야 한다.\n",
    "\n",
    "6. reshape 이용해서 create sequence 부분 고치기\n",
    "\n",
    "7. 실제 연구 데이터를 사용할 때, 두 도메인의 데이터 길이 조심하기 - 학습과정에서\n",
    "\n",
    "8. notes_to_midi 잘 이용하기\n",
    "\n",
    "9. MIDI file 제대로, 전부 append 되는 지 확인하기 --> file_path glob 부분에서 slicing에 의한 문제 --> 해결\n",
    "\n",
    "10. cycleGAN 생성 과정 중에 pitch, step, duration의 loss 차이가 튼 영향을 미친다면 music_generation 노트북을 활용하여 loss 재정의하기\n",
    "\n",
    "11. 각 노래 midi 별로 나눠서 처리하거나, batch size 크기를 늘려 모델이 batch로 학습을 진행할 때, 각각의 노래가 섞이는 경우를 조금은 줄이는 것이 좋은지 실험 필요\n",
    "\n",
    "12. colab 환경에서 여러개의 notebook을 띄워 놓은 후, 각각의 변수들을 변경하면서 실험 진행<br>\n",
    "\n",
    "13. 12를 진행하는 과정에서 실험 리스트, 설계 과정, 결과 등을 모두 기록해야함\n",
    "\n",
    "14. colab notebook 최대 가용량 -> 실험 가능량 설계 -> 시간 계획\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment plan\n",
    "1. batch\n",
    "2. data<br>\n",
    "    1. count\n",
    "    2. type<br>\n",
    "        1. musician<br>\n",
    "            1. piano-e-competition\n",
    "        2. elec<br>\n",
    "            1. maestro\n",
    "3. generator model<br>\n",
    "    1. model structure<br>\n",
    "        1. type<br>\n",
    "            1. bidirectional\n",
    "            2. Dense\n",
    "            3. GRU\n",
    "        2. layer<br>\n",
    "            1. unit count\n",
    "    2. hyper parameter\n",
    "4. discriminator model\n",
    "    1. model structure<br>\n",
    "        1. type<br>\n",
    "            1. bidirectional\n",
    "            2. Dense\n",
    "            3. GRU\n",
    "        2. layer<br>\n",
    "            1. unit count\n",
    "    2. hyper parameter<br>\n",
    "    3. for music<br>\n",
    "        1. patchGAN\n",
    "5. loss function weight<br>\n",
    "    1. brute force method\n",
    "6. optimizer<br>\n",
    "    1. type\n",
    "    2. learning rate\n",
    "7. future plan<br>\n",
    "    1. 특정곡의 곡 또는 작곡관련 nlp 정보 -> embbding -> 새로운 층으로 concatenate -> multimodal<br>\n",
    "    2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for installing\n",
    "\n",
    "# !pip install -U -q pip\n",
    "# !pip install -q tensorboardX\n",
    "# !pip install -q git+https://github.com/tensorflow/examples.git\n",
    "# !pip install -q pretty_midi\n",
    "# !pip install -q knockknock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'piano-e-competition-midi-files' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/chelate1118/piano-e-competition-midi-files.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'GiantMIDI-Piano'...\n",
      "Updating files:  97% (287/294)\n",
      "Updating files:  98% (289/294)\n",
      "Updating files:  99% (292/294)\n",
      "Updating files: 100% (294/294)\n",
      "Updating files: 100% (294/294), done.\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/bytedance/GiantMIDI-Piano.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gobak\\.virtualenvs\\.venv-HOEqjjdC\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import 과정 error -> AVX2를 이용한 tf compile 진행 시, 더 최적화 가능함을 알려줌 | 아래 링크 참조\n",
    "# http://erpcomputing.com/build-tensorflow-from-source--mkl-enabled\n",
    "\n",
    "#library import\n",
    "\n",
    "# for data and preprocessing\n",
    "import tensorflow_datasets as tfds\n",
    "import collections\n",
    "import glob\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import pretty_midi\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import MIDI_Pedal # 사용법 : MIDI_Pedal.apply('test.mid', 'output.mid')\n",
    "\n",
    "\n",
    "# for model\n",
    "# 설치가 git인 경우 local에서 설치한 후 .venv/Lib에 넣으면 됨, local pip 설치경로는 위에 있음\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, GRU, LeakyReLU, Activation, TimeDistributed\n",
    "from keras.layers import TimeDistributed, GRU, Dense, Dropout, Conv2D, BatchNormalization, MaxPool2D, GlobalMaxPool2D\n",
    "import keras \n",
    "\n",
    "# for the others\n",
    "# import fluidsynth # MIDI는 다른 거로 재생하기\n",
    "import datetime\n",
    "from IPython import display\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Optional, Sequence, Tuple\n",
    "import time\n",
    "import os\n",
    "# from knockknock import desktop_sender\n",
    "# from google.colab import files # google colab에서 결과 midi file 다운받고 싶을 때 import 하면 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295\n"
     ]
    }
   ],
   "source": [
    "# archive_data_dir = pathlib.Path('archive')\n",
    "# data_l = len(glob.glob(str(archive_data_dir/'**/*.mid*')))\n",
    "# print(data_l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files : 10854\n",
      "giant_midis\\A., Jag, Je t'aime Juliette, OXC7Fd0ZN8o.mid\n"
     ]
    }
   ],
   "source": [
    "# giant midi piano dataset\n",
    "\n",
    "giantmidi_data_dir = pathlib.Path('giant_midis')\n",
    "giantmidi_filenames = glob.glob(str(giantmidi_data_dir/'*.mid*'))\n",
    "print('Number of files :', len(giantmidi_filenames))\n",
    "\n",
    "giantmidi_sample_file = giantmidi_filenames[0]\n",
    "print(giantmidi_sample_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files : 1282\n",
      "data\\maestro-v2.0.0\\2004\\MIDI-Unprocessed_XP_15_R2_2004_01_ORIG_MID--AUDIO_15_R2_2004_03_Track03_wav.midi\n"
     ]
    }
   ],
   "source": [
    "# maestro dataset\n",
    "\n",
    "maestro_data_dir = pathlib.Path('data/maestro-v2.0.0')\n",
    "if not maestro_data_dir.exists():\n",
    "  tf.keras.utils.get_file(\n",
    "      'maestro-v2.0.0-midi.zip',\n",
    "      origin='https://storage.googleapis.com/magentadata/datasets/maestro/v2.0.0/maestro-v2.0.0-midi.zip',\n",
    "      extract=True,\n",
    "      cache_dir='.', cache_subdir='data',\n",
    "  )\n",
    "\n",
    "maestro_filenames = glob.glob(str(maestro_data_dir/'**/*.mid*'))\n",
    "print('Number of files :', len(maestro_filenames))\n",
    "\n",
    "maestro_sample_file = maestro_filenames[0]\n",
    "print(maestro_sample_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instruments: 1\n",
      "Instrument name: Acoustic Grand Piano\n"
     ]
    }
   ],
   "source": [
    "pm = pretty_midi.PrettyMIDI(maestro_sample_file)\n",
    "pm_test = pretty_midi.PrettyMIDI(giantmidi_sample_file)\n",
    "\n",
    "instrument = pm.instruments[0]\n",
    "instrument_test = pm_test.instruments[0]\n",
    "instrument_name = pretty_midi.program_to_instrument_name(instrument.program)\n",
    "instrument_name_test = pretty_midi.program_to_instrument_name(instrument_test.program)\n",
    "\n",
    "if not instrument_name == instrument_name_test:\n",
    "    raise Exception('both dataset has incompatiable instrument')\n",
    "\n",
    "print('Number of instruments:', len(pm.instruments))\n",
    "print('Instrument name:', instrument_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variable declaration\n",
    "\n",
    "#global\n",
    "seed = 42\n",
    "\n",
    "# for model\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "tf.random.set_seed(seed)\n",
    "BUFFER_SIZE = -1\n",
    "BATCH_SIZE = 1 # cycleGAN tf document 참고한 값 \n",
    "SEQ_LENGTH = 1024\n",
    "VOCAB_SIZE = 128\n",
    "\n",
    "N_FEATURE = OUTPUT_CHANNELS = 3\n",
    "LR_gen = 2e-4 # opt\n",
    "LR_disc = 2e-4\n",
    "# B1 = 0.05 # opt\n",
    "\n",
    "# for data\n",
    "np.random.seed(seed)\n",
    "# MUSICIAN_MIDI_FILES_DIR = pathlib.Path('data/maestro-v2.0.0')\n",
    "MUSICIAN_MIDI_FILES_DIR = pathlib.Path('data/maestro-v2.0.0')\n",
    "ELEC_MIDI_FILES_DIR = pathlib.Path('giant_midis')\n",
    "DATA_NUM = 1000\n",
    "\n",
    "# for the others\n",
    "# Sampling rate for audio playback\n",
    "_SAMPLING_RATE = 16000\n",
    "PRINT_ITER = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "용어 정리<br>real : 실제 연주자 연주<br>elec : 전자 음악 연주"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def midi_to_notes(midi_file: str) -> pd.DataFrame:\n",
    "  pm = pretty_midi.PrettyMIDI(midi_file)\n",
    "  instrument = pm.instruments[0]\n",
    "  notes = collections.defaultdict(list)\n",
    "\n",
    "  # Sort the notes by start time\n",
    "  sorted_notes = sorted(instrument.notes, key=lambda note: note.start)\n",
    "  prev_start = sorted_notes[0].start\n",
    "\n",
    "  for note in sorted_notes:\n",
    "    start = note.start\n",
    "    end = note.end\n",
    "    notes['pitch'].append(note.pitch)\n",
    "    notes['start'].append(start)\n",
    "    notes['end'].append(end)\n",
    "    notes['step'].append(start - prev_start)\n",
    "    notes['duration'].append(end - start)\n",
    "    prev_start = start\n",
    "\n",
    "  return pd.DataFrame({name: (np.array(value) if name == 'pitch' else np.float32(np.array(value))) for name, value in notes.items()})\n",
    "\n",
    "def plot_piano_roll(notes: pd.DataFrame, count: Optional[int] = None):\n",
    "    if count:\n",
    "      title = f'First {count} notes'\n",
    "    else:\n",
    "      title = f'Whole track'\n",
    "      count = len(notes['pitch'])\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    plot_pitch = np.stack([notes['pitch'], notes['pitch']], axis=0)\n",
    "    plot_start_stop = np.stack([notes['start'], notes['end']], axis=0)\n",
    "    plt.plot(\n",
    "        plot_start_stop[:, :count], plot_pitch[:, :count], color=\"b\", marker=\".\")\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('Pitch')\n",
    "    _ = plt.title(title)\n",
    "\n",
    "def plot_distributions(notes: pd.DataFrame, drop_percentile=2.5):\n",
    "  plt.figure(figsize=[15, 5])\n",
    "  plt.subplot(1, 3, 1)\n",
    "  sns.histplot(notes, x=\"pitch\", bins=20)\n",
    "\n",
    "  plt.subplot(1, 3, 2)\n",
    "  max_step = np.percentile(notes['step'], 100 - drop_percentile)\n",
    "  sns.histplot(notes, x=\"step\", bins=np.linspace(0, max_step, 21))\n",
    "  \n",
    "  plt.subplot(1, 3, 3)\n",
    "  max_duration = np.percentile(notes['duration'], 100 - drop_percentile)\n",
    "  sns.histplot(notes, x=\"duration\", bins=np.linspace(0, max_duration, 21))\n",
    "\n",
    "def notes_to_midi(\n",
    "  notes: pd.DataFrame,\n",
    "  out_file: str, \n",
    "  instrument_name: str,\n",
    "  velocity: int = 100,  # note loudness\n",
    ") -> pretty_midi.PrettyMIDI:\n",
    "\n",
    "  pm = pretty_midi.PrettyMIDI()\n",
    "  instrument = pretty_midi.Instrument(\n",
    "      program=pretty_midi.instrument_name_to_program(\n",
    "          instrument_name))\n",
    "\n",
    "  prev_start = 0\n",
    "  for i, note in notes.iterrows():\n",
    "    start = float(prev_start + note['step'])\n",
    "    end = float(start + note['duration'])\n",
    "    note = pretty_midi.Note(\n",
    "        velocity=velocity,\n",
    "        pitch=int(note['pitch']),\n",
    "        start=start,\n",
    "        end=end,\n",
    "    )\n",
    "    instrument.notes.append(note)\n",
    "    prev_start = start\n",
    "\n",
    "  pm.instruments.append(instrument)\n",
    "  pm.write(out_file)\n",
    "  return pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 참고\n",
    "<span style='font-size:80%'>1. midi_to_notes(midi_file: str) -> pd.DataFrame<br>2. notes_to_midi(\n",
    "  notes: pd.DataFrame,\n",
    "  out_file: str, \n",
    "  instrument_name: str,\n",
    "  velocity: int = 100,  # note loudness\n",
    ") -> pretty_midi.PrettyMIDI<br>3. plot_piano_roll(notes: pd.DataFrame, count: Optional[int] = None)<br>4. def plot_distributions(notes: pd.DataFrame, drop_percentile=2.5)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_NUM = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process 50.000% 10th file process continue...\n",
      "\n",
      "End!\n",
      "total process time for 20 files : 3.434sec\n"
     ]
    }
   ],
   "source": [
    "# data import musician AND FAKE\n",
    "# error 많이 나면 np.array 이용하기\n",
    "# filenames slicing 수 적절히 설정하기\n",
    "\n",
    "# musician_filenames = glob.glob(str(MUSICIAN_MIDI_FILES_DIR/'**/*.mid*'))\n",
    "musician_filenames = glob.glob(str(MUSICIAN_MIDI_FILES_DIR/'**/*.mid*'))\n",
    "# musician => 2700개 정도 있음\n",
    "musician_filenames = musician_filenames[:DATA_NUM]\n",
    "\n",
    "elec_filenames = glob.glob(str(ELEC_MIDI_FILES_DIR/'*.mid*'))\n",
    "elec_filenames = elec_filenames[:DATA_NUM]\n",
    "\n",
    "print(len(musician_filenames))\n",
    "print(len(elec_filenames))\n",
    "\n",
    "\n",
    "musician_all_notes = []\n",
    "elec_all_notes = []\n",
    "\n",
    "# 각 notes listz\n",
    "# 0 : musician - 사람이 연주한 데이터\n",
    "# 1 : fake - 사람이 연주하지 않은 데이터\n",
    "data_notes = [musician_all_notes, elec_all_notes]\n",
    "data_paths = [musician_filenames, elec_filenames]\n",
    "data_notes_ds = {} # tfds를 이용한 slices 이후 데이터임, (n_notes, tf_dt) tuple형의 데이터가 원소인 list\n",
    "\n",
    "train_data_notes_ds = [] # n_notes, dataset(slice)의 tuple 형태\n",
    "validation_data_notes_ds = []\n",
    "test_data_notes_ds = []\n",
    "\n",
    "data_split_ratio = {'train' : 0.8, 'validation' : 0.2, 'test' : 0.2}\n",
    "\n",
    "key_order = ['pitch', 'step', 'duration']\n",
    "not_key_order = ['start', 'end']\n",
    "\n",
    "data_type = ['musician', 'elec']\n",
    "\n",
    "# notes 수를 맞추기 위함\n",
    "n_notes = 10e9\n",
    "\n",
    "file_cnt = 0\n",
    "\n",
    "print(f'{len(musician_filenames) + len(elec_filenames)} files are ready to process')\n",
    "print('-' * PRINT_ITER)\n",
    "print()\n",
    "print('Start!')\n",
    "print()\n",
    "time.sleep(2)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i, data_path in enumerate(data_paths):\n",
    "    for f in data_path:\n",
    "        notes = midi_to_notes(f)\n",
    "        data_notes[i].append(notes)\n",
    "        if file_cnt % 10 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f'process {(file_cnt / (len(musician_filenames) + len(elec_filenames))) * 100.0:.3f}% {file_cnt}th file process continue...')\n",
    "        file_cnt += 1\n",
    "        \n",
    "    # append와 concat은 다르다!\n",
    "    data_notes[i] = pd.concat(data_notes[i])\n",
    "    data_notes[i].drop(columns=not_key_order, inplace=True)\n",
    "    n_notes = min(len(data_notes[i]), n_notes)    \n",
    "    \n",
    "for i, data_path in enumerate(data_paths):\n",
    "    _train, _test = data_notes[i][:int(n_notes * (1 - data_split_ratio['test']))], data_notes[i][int(n_notes * (1 - data_split_ratio['test'])):]\n",
    "    _train, _validation = _train[:int(len(_train) * data_split_ratio['train'])], _train[int(len(_train) * data_split_ratio['train']):]\n",
    "    n__train = len(_train)\n",
    "    n__validation = len(_validation)\n",
    "    n__test = len(_test)\n",
    "    data_dict = {'train' : (n__train, _train), 'validation' : (n__validation, _validation), 'test' : (n__test, _test)}\n",
    "    data_notes_ds[data_type[i]] = data_dict\n",
    "    \n",
    "    \n",
    "print()\n",
    "print('End!')\n",
    "\n",
    "print(f'total process time for {len(musician_filenames) + len(elec_filenames)} files : {time.time() - start_time:.3f}sec')\n",
    "# data_notes_ds Info\n",
    "# (n_data, tfds) 원소,\n",
    "# musician ==> train, validation, test\n",
    "# elec ==> train, validation, test\n",
    "# 총 6개의 원소가 들어가 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pitch       False\n",
       "step        False\n",
       "duration    False\n",
       "dtype: bool"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_notes_ds['musician']['train'][1].isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.000000\n",
       "1       0.186458\n",
       "2       0.009375\n",
       "3       0.175000\n",
       "4       0.169792\n",
       "          ...   \n",
       "8072    0.003125\n",
       "8073    0.011458\n",
       "8074    0.014583\n",
       "8075    0.015625\n",
       "8076    0.197917\n",
       "Name: step, Length: 26371, dtype: float32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_notes_ds['musician']['train'][1]['step']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pitch</th>\n",
       "      <th>step</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.096875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55</td>\n",
       "      <td>0.186458</td>\n",
       "      <td>0.217708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>71</td>\n",
       "      <td>0.009375</td>\n",
       "      <td>0.505208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.167708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62</td>\n",
       "      <td>0.169792</td>\n",
       "      <td>0.119792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8072</th>\n",
       "      <td>72</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.042708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8073</th>\n",
       "      <td>74</td>\n",
       "      <td>0.011458</td>\n",
       "      <td>0.072917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8074</th>\n",
       "      <td>69</td>\n",
       "      <td>0.014583</td>\n",
       "      <td>0.037500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8075</th>\n",
       "      <td>57</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.038542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8076</th>\n",
       "      <td>43</td>\n",
       "      <td>0.197917</td>\n",
       "      <td>0.044792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26371 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pitch      step  duration\n",
       "0        71  0.000000  0.096875\n",
       "1        55  0.186458  0.217708\n",
       "2        71  0.009375  0.505208\n",
       "3        59  0.175000  0.167708\n",
       "4        62  0.169792  0.119792\n",
       "...     ...       ...       ...\n",
       "8072     72  0.003125  0.042708\n",
       "8073     74  0.011458  0.072917\n",
       "8074     69  0.014583  0.037500\n",
       "8075     57  0.015625  0.038542\n",
       "8076     43  0.197917  0.044792\n",
       "\n",
       "[26371 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_notes_ds['musician']['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pitch</th>\n",
       "      <th>step</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.506510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84</td>\n",
       "      <td>0.001302</td>\n",
       "      <td>0.505208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48</td>\n",
       "      <td>0.009115</td>\n",
       "      <td>0.006510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.295573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52</td>\n",
       "      <td>0.143229</td>\n",
       "      <td>0.513021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4108</th>\n",
       "      <td>66</td>\n",
       "      <td>0.052083</td>\n",
       "      <td>0.023438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4109</th>\n",
       "      <td>93</td>\n",
       "      <td>0.179688</td>\n",
       "      <td>0.036458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4110</th>\n",
       "      <td>78</td>\n",
       "      <td>0.022135</td>\n",
       "      <td>0.029948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4111</th>\n",
       "      <td>74</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4112</th>\n",
       "      <td>95</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.148438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26371 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pitch      step  duration\n",
       "0        45  0.000000  0.506510\n",
       "1        84  0.001302  0.505208\n",
       "2        48  0.009115  0.006510\n",
       "3        76  0.000000  0.295573\n",
       "4        52  0.143229  0.513021\n",
       "...     ...       ...       ...\n",
       "4108     66  0.052083  0.023438\n",
       "4109     93  0.179688  0.036458\n",
       "4110     78  0.022135  0.029948\n",
       "4111     74  0.000000  0.063802\n",
       "4112     95  0.015625  0.148438\n",
       "\n",
       "[26371 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_notes_ds['elec']['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 26371 entries, 0 to 4112\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   pitch     26371 non-null  int32  \n",
      " 1   step      26371 non-null  float32\n",
      " 2   duration  26371 non-null  float32\n",
      "dtypes: float32(2), int32(1)\n",
      "memory usage: 515.1 KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 26371 entries, 0 to 8076\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   pitch     26371 non-null  int32  \n",
      " 1   step      26371 non-null  float32\n",
      " 2   duration  26371 non-null  float32\n",
      "dtypes: float32(2), int32(1)\n",
      "memory usage: 515.1 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# input data nan or inf 확인\n",
    "# min, max 값 확인해서 이상치 확인하기\n",
    "\n",
    "#TODO\n",
    "# pitch도 Dtype float64로 바꿔보기 / 아니면 전부 float32로 굳이 64까지 안 써도 무방\n",
    "print(data_notes_ds['elec']['train'][1].info())\n",
    "print(data_notes_ds['musician']['train'][1].info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              pitch          step      duration\n",
      "count  26371.000000  26371.000000  26371.000000\n",
      "mean      64.831444      0.143615      0.540759\n",
      "std       12.745089      0.413116      0.644795\n",
      "min       24.000000      0.000000      0.003906\n",
      "25%       56.000000      0.002604      0.136719\n",
      "50%       64.000000      0.016927      0.333333\n",
      "75%       74.000000      0.165365      0.670573\n",
      "max      103.000000     15.378906      6.005208\n",
      "              pitch          step      duration\n",
      "count  26371.000000  26371.000000  26371.000000\n",
      "mean      64.845323      0.086609      0.165801\n",
      "std       12.251832      0.170984      0.307159\n",
      "min       24.000000      0.000000      0.001042\n",
      "25%       55.000000      0.007292      0.051042\n",
      "50%       66.000000      0.036458      0.086458\n",
      "75%       74.000000      0.126042      0.156250\n",
      "max      105.000000      8.341666      6.515625\n"
     ]
    }
   ],
   "source": [
    "print(data_notes_ds['elec']['train'][1].describe())\n",
    "print(data_notes_ds['musician']['train'][1].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cycleGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# many to many에 맞게 gru 사용하기, timedistributed and reshape와 같은 함수 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v2\n",
    "def get_generator(model_name, output_channels=OUTPUT_CHANNELS):\n",
    "  input_shape = (SEQ_LENGTH, output_channels)\n",
    "  # print(input_shape)\n",
    "  \n",
    "  # LSTM, Bi를 아래와 같이 단순히 쌓아도 되는지 불분명\n",
    "  # onhot encoding이나 embedding처럼 차원 조절 방법 생각\n",
    "  \n",
    "  inputs = tf.keras.Input(input_shape, name='gen_input')\n",
    "  x0 = Bidirectional(LSTM(3, return_sequences=True))(inputs)\n",
    "  x1 = Bidirectional(LSTM(6, return_sequences=True))(x0)\n",
    "  \n",
    "  x1 = Bidirectional(LSTM(12, return_sequences=True))(x1)\n",
    "  x1 = Bidirectional(LSTM(12, return_sequences=True))(x1)\n",
    "  \n",
    "  x1 = Bidirectional(LSTM(24, return_sequences=True))(x1)\n",
    "  x1 = Bidirectional(LSTM(24, return_sequences=True))(x1)\n",
    "  \n",
    "  x1 = Bidirectional(LSTM(12, return_sequences=True))(x1)\n",
    "  x1 = Bidirectional(LSTM(12, return_sequences=True))(x1)\n",
    "\n",
    "  x1 = Bidirectional(LSTM(6, return_sequences=True))(x1)\n",
    "  x2 = Bidirectional(LSTM(3, return_sequences=True))(x1)\n",
    "  \n",
    "  outputs = TimeDistributed(Dense(output_channels), name='gen_output')(x2)\n",
    "  # 마지막 layer는 timedistributed layer로! 다른 거도 가능하면 고려하기\n",
    "  model = tf.keras.Model(inputs, outputs, name=model_name)\n",
    "  return model\n",
    "\n",
    "\n",
    "# 향후 수정 후, 사용하기\n",
    "def get_discriminator(model_name, output_channels=OUTPUT_CHANNELS):\n",
    "  input_shape = (SEQ_LENGTH, output_channels)\n",
    "  # print(input_shape)\n",
    "  inputs = tf.keras.Input(input_shape, name='disc_input')\n",
    "  x0 = Bidirectional(LSTM(3, return_sequences=True))(inputs)\n",
    "  \n",
    "  x1 = Bidirectional(LSTM(6, return_sequences=True))(x0)\n",
    "  x1 = Bidirectional(LSTM(12, return_sequences=True))(x1)\n",
    "  x1 = Bidirectional(LSTM(12, return_sequences=True))(x1)\n",
    "  x1 = Bidirectional(LSTM(12, return_sequences=True))(x1)\n",
    "\n",
    "  x2 = Bidirectional(LSTM(12, return_sequences=True))(x1)\n",
    "  \n",
    "  # activation='sigmoid'를 loss nan 문제 해결방법으로 사용해보기\n",
    "  outputs = TimeDistributed(Dense(24), name='disc_output')(x2)\n",
    "  \n",
    "  # 마지막 layer는 timedistributed layer로! 다른 거도 가능하면 고려하기\n",
    "  model = tf.keras.Model(inputs, outputs, name=model_name)\n",
    "  return model\n",
    "    \n",
    "\n",
    "# 향후 종류를 다양하게 변형하기 / 여기에서 pretrained model 이용함\n",
    "# gen_musician = tf.keras.models.load_model('model_save/v17-2/gen_elec-epoch2.h5')\n",
    "# gen_elec = tf.keras.models.load_model('model_save/v17-2/gen_elec-epoch2.h5')\n",
    "gen_musician = get_generator('gen_musician')\n",
    "gen_elec = get_generator('gen_elec')\n",
    "\n",
    "disc_musician = get_discriminator('disc_musician')\n",
    "disc_elec = get_discriminator('disc_elec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_convnet(shape=(112, 112, 3)):\n",
    "    momentum = .9\n",
    "    model = keras.Sequential()\n",
    "    model.add(Conv2D(64, (3,3), input_shape=shape,\n",
    "        padding='same', activation='relu'))\n",
    "    model.add(Conv2D(64, (3,3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=momentum))\n",
    "    \n",
    "    model.add(MaxPool2D())\n",
    "    \n",
    "    model.add(Conv2D(128, (3,3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(128, (3,3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=momentum))\n",
    "    \n",
    "    model.add(MaxPool2D())\n",
    "    \n",
    "    model.add(Conv2D(256, (3,3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(256, (3,3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=momentum))\n",
    "    \n",
    "    model.add(MaxPool2D())\n",
    "    \n",
    "    model.add(Conv2D(512, (3,3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(512, (3,3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=momentum))\n",
    "    \n",
    "    # flatten...\n",
    "    model.add(GlobalMaxPool2D())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_16 (Conv2D)          (None, 112, 112, 64)      1792      \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 112, 112, 64)      36928     \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 112, 112, 64)     256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 56, 56, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 56, 56, 128)       73856     \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 56, 56, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 56, 56, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 28, 28, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_20 (Conv2D)          (None, 28, 28, 256)       295168    \n",
      "                                                                 \n",
      " conv2d_21 (Conv2D)          (None, 28, 28, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 28, 28, 256)      1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 14, 14, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_22 (Conv2D)          (None, 14, 14, 512)       1180160   \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 14, 14, 512)      2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " global_max_pooling2d_2 (Glo  (None, 512)              0         \n",
      " balMaxPooling2D)                                                \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,689,216\n",
      "Trainable params: 4,687,296\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "b = build_convnet()\n",
    "b.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_model(shape=(5, 112, 112, 3), nbout=3):\n",
    "    # Create our convnet with (112, 112, 3) input shape\n",
    "    convnet = build_convnet(shape[1:])\n",
    "    \n",
    "    # then create our final model\n",
    "    model = keras.Sequential()\n",
    "    # add the convnet with (5, 112, 112, 3) shape\n",
    "    model.add(TimeDistributed(convnet, input_shape=shape))\n",
    "    # here, you can also use GRU or LSTM\n",
    "    model.add(GRU(64))\n",
    "    # and finally, we make a decision network\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(nbout, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = action_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_1 (TimeDis  (None, 5, 512)           4689216   \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 64)                110976    \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 1024)              66560     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 128)               65664     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,465,667\n",
      "Trainable params: 5,463,747\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "a.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"gen_musician\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gen_input (InputLayer)      [(None, 1024, 3)]         0         \n",
      "                                                                 \n",
      " bidirectional_32 (Bidirecti  (None, 1024, 6)          168       \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_33 (Bidirecti  (None, 1024, 12)         624       \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_34 (Bidirecti  (None, 1024, 24)         2400      \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_35 (Bidirecti  (None, 1024, 24)         3552      \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_36 (Bidirecti  (None, 1024, 48)         9408      \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_37 (Bidirecti  (None, 1024, 48)         14016     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_38 (Bidirecti  (None, 1024, 24)         5856      \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_39 (Bidirecti  (None, 1024, 24)         3552      \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_40 (Bidirecti  (None, 1024, 12)         1488      \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_41 (Bidirecti  (None, 1024, 6)          384       \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " gen_output (TimeDistributed  (None, 1024, 3)          21        \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 41,469\n",
      "Trainable params: 41,469\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gen_musician.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"disc_musician\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " disc_input (InputLayer)     [(None, 1024, 3)]         0         \n",
      "                                                                 \n",
      " bidirectional_52 (Bidirecti  (None, 1024, 6)          168       \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_53 (Bidirecti  (None, 1024, 12)         624       \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_54 (Bidirecti  (None, 1024, 24)         2400      \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_55 (Bidirecti  (None, 1024, 24)         3552      \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_56 (Bidirecti  (None, 1024, 24)         3552      \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_57 (Bidirecti  (None, 1024, 24)         3552      \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " disc_output (TimeDistribute  (None, 1024, 24)         600       \n",
      " d)                                                              \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,448\n",
      "Trainable params: 14,448\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "disc_musician.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "\n",
    "loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "# losses_ratio = {'disc_loss' : 1.0, 'gen_loss' : 1.0, 'cycle_loss' : 1.0}\n",
    "\n",
    "for_nan_checking = {}\n",
    "for_prev_nan_checking = {}\n",
    "\n",
    "# first_one_checked = False\n",
    "nan_checked = False\n",
    "\n",
    "def disc_loss(real, generated):\n",
    "#     real += tf.convert_to_tensor(1)\n",
    "#     real = tf.math.scalar_mul(0.5, real)\n",
    "    \n",
    "#     generated += tf.convert_to_tensor(1)\n",
    "#     generated = tf.math.scalar_mul(0.5, real)\n",
    "    global for_nan_checking, for_prev_nan_checking, nan_checked\n",
    "    \n",
    "    real_loss = loss_obj(tf.ones_like(real), real)\n",
    "    if real_loss != real_loss:\n",
    "        for_nan_checking['real_loss'] = real\n",
    "        nan_checked = True\n",
    "    \n",
    "    if not nan_checked:\n",
    "        for_prev_nan_checking['real_loss'] = real\n",
    "        \n",
    "    \n",
    "    fake_loss = loss_obj(tf.zeros_like(generated), generated)\n",
    "    if fake_loss != fake_loss:\n",
    "        for_nan_checking['fake_loss'] = generated\n",
    "        nan_checked = True\n",
    "        \n",
    "    if not nan_checked:\n",
    "        for_prev_nan_checking['fake_loss'] = generated\n",
    "    \n",
    "    total_loss = real_loss + fake_loss\n",
    "    \n",
    "    \n",
    "    return total_loss * 0.5\n",
    "\n",
    "def gen_loss(generated):\n",
    "#     generated += tf.convert_to_tensor(1)\n",
    "#     generated = tf.math.scalar_mul(0.5, generated)\n",
    "    global for_nan_checking, for_prev_nan_checking, nan_checked\n",
    "\n",
    "    t_gen_loss = loss_obj(tf.ones_like(generated), generated)\n",
    "    \n",
    "    if t_gen_loss != t_gen_loss:\n",
    "        for_nan_checking['gen_loss'] = generated\n",
    "        nan_checked = True\n",
    "        \n",
    "    if not nan_checked:\n",
    "        for_prev_nan_checking['gen_loss'] = generated\n",
    "        \n",
    "    return t_gen_loss * 1\n",
    "\n",
    "def cycle_loss(real_data, cycled_data):\n",
    "    global for_nan_checking, for_prev_nan_checking, nan_checked\n",
    "    \n",
    "    t_cycle_loss = tf.reduce_mean(tf.abs(real_data - cycled_data))\n",
    "    \n",
    "    if t_cycle_loss != t_cycle_loss:\n",
    "        for_nan_checking['cycle_loss'] = (real_data, cycled_data)\n",
    "        nan_checked = True\n",
    "        \n",
    "    if not nan_checked:\n",
    "        for_prev_nan_checking['cycle_loss'] = (real_data, cycled_data)\n",
    "        \n",
    "    return t_cycle_loss * 10\n",
    "\n",
    "def iden_loss(real_data, gen_from_real_data):\n",
    "    global for_nan_checking, for_prev_nan_checking, nan_checked\n",
    "    \n",
    "    t_iden_loss = tf.reduce_mean(tf.abs(real_data - gen_from_real_data))\n",
    "    \n",
    "    if t_iden_loss != t_iden_loss:\n",
    "        for_nan_checking['iden_loss'] = (real_data, gen_from_real_data)\n",
    "        nan_checked = True\n",
    "        \n",
    "    if not nan_checked:\n",
    "        for_prev_nan_checking['iden_loss'] = (real_data, gen_from_real_data)\n",
    "    \n",
    "    return t_iden_loss * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "\n",
    "# LR_gen = 9e-4\n",
    "# LR_disc = 1e-6\n",
    "# B1 = 0.5 \n",
    "\n",
    "gen_musician_opt = tf.keras.optimizers.Adam(learning_rate=LR_gen)\n",
    "gen_elec_opt = tf.keras.optimizers.Adam(learning_rate=LR_gen)\n",
    "\n",
    "disc_musician_opt = tf.keras.optimizers.Adam(learning_rate=LR_disc)\n",
    "disc_elec_opt = tf.keras.optimizers.Adam(learning_rate=LR_disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # checkpoint_path의 경우 계속해서 바꿔줘야 에러가 나지 않음, 이 cell(+위에 model 정의 cell)을 반복해서 실행하면 error 발생함\n",
    "# checkpoint_path = \"./checkpoint/train_2022_11_10\"\n",
    "\n",
    "# ckpt = tf.train.Checkpoint(gen_musician=gen_musician,\n",
    "#                            gen_elec=gen_elec,\n",
    "#                            disc_musician=disc_musician,\n",
    "#                            disc_elec=disc_elec,\n",
    "#                            gen_musician_opt=gen_musician_opt,\n",
    "#                            gen_elec_opt=gen_elec_opt,\n",
    "#                            disc_musician_opt=disc_musician_opt,\n",
    "#                            disc_elec_opt=disc_elec_opt)\n",
    "\n",
    "# ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# # if a checkpoint exists, restore the latest checkpoint.\n",
    "# if ckpt_manager.latest_checkpoint:\n",
    "#   ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "#   print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real_x : musician data\n",
    "# real_y : elec data\n",
    "# g : gen elec\n",
    "# f : gen music\n",
    "\n",
    "@tf.function\n",
    "def train_step(real_x, real_y, losses_ratio):\n",
    "    # persistent is set to True because the tape is used more than\n",
    "    # once to calculate the gradients.\n",
    "    all_loss = []\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # Generator G translates X -> Y\n",
    "        # Generator F translates Y -> X.\n",
    "        fake_y = gen_elec(real_x, training=True)\n",
    "        cycled_x = gen_musician(fake_y, training=True)\n",
    "\n",
    "        fake_x = gen_musician(real_y, training=True)\n",
    "        cycled_y = gen_elec(fake_x, training=True)\n",
    "\n",
    "        # same_x and same_y are used for identity loss.\n",
    "        same_x = gen_musician(real_x, training=True)\n",
    "        same_y = gen_elec(real_y, training=True)\n",
    "        \n",
    "        real_x_pitch = np.round(real_x.numpy()[:, :, 0] * 128)\n",
    "        fake_y_pitch = np.round(fake_y.numpy()[:, :, 0] * 128)\n",
    "        \n",
    "        real_y_pitch = np.round(real_y.numpy()[:, :, 0] * 128)\n",
    "        fake_x_pitch = np.round(fake_x.numpy()[:, :, 0] * 128)\n",
    "        \n",
    "        # print(real_x_pitch[0], fake_y_pitch[0])\n",
    "        \n",
    "#         print(type((real_x_pitch == fake_y_pitch)))\n",
    "        false_pitch_sum = np.sum(np.abs(real_x_pitch - fake_y_pitch)) + np.sum(np.abs(real_y_pitch - fake_x_pitch))\n",
    "        \n",
    "        false_pitch_sum = np.sqrt(false_pitch_sum).item()\n",
    "        \n",
    "        # 모든 pitch가 같아, false_pitch_sum == 0인 경우 방지 \n",
    "        false_pitch_sum = false_pitch_sum if false_pitch_sum > 1.0 else 1\n",
    "        \n",
    "#         print(false_pitch_sum)\n",
    "        \n",
    "        \n",
    "#         print(real_x_pitch)\n",
    "        \n",
    "        # print(real_x_pitch == fake_y_pitch)\n",
    "        \n",
    "        \n",
    "#         print(real_x[0].numpy()[:, 0] == same_x[0].numpy()[:, 0])\n",
    "#         print(type(same_x))\n",
    "#         print(same_x.numpy())\n",
    "        \n",
    "\n",
    "        disc_real_x = disc_musician(real_x, training=True)\n",
    "        disc_real_y = disc_elec(real_y, training=True)\n",
    "\n",
    "        disc_fake_x = disc_musician(fake_x, training=True)\n",
    "        disc_fake_y = disc_elec(fake_y, training=True)\n",
    "\n",
    "        # calculate the loss\n",
    "        gen_elec_loss = gen_loss(disc_fake_y) #* false_pitch_sum\n",
    "        gen_musician_loss = gen_loss(disc_fake_x) #* false_pitch_sum\n",
    "        total_cycle_loss = cycle_loss(real_x, cycled_x) + cycle_loss(real_y, cycled_y)\n",
    "\n",
    "        elec_iden_loss = iden_loss(real_y, same_y)\n",
    "        musician_iden_loss = iden_loss(real_x, same_x)\n",
    "\n",
    "        # Total generator loss = adversarial loss + cycle loss\n",
    "        total_gen_elec_loss = gen_elec_loss*losses_ratio['gen'] + total_cycle_loss*losses_ratio['cyc'] + elec_iden_loss*losses_ratio['iden']\n",
    "        total_gen_musician_loss = gen_musician_loss*losses_ratio['gen'] + total_cycle_loss*losses_ratio['cyc'] + musician_iden_loss*losses_ratio['iden']\n",
    "\n",
    "        disc_musician_loss = disc_loss(disc_real_x, disc_fake_x)*losses_ratio['disc']\n",
    "        disc_elec_loss = disc_loss(disc_real_y, disc_fake_y)*losses_ratio['disc']\n",
    "\n",
    "        all_loss = [tf.identity(gen_elec_loss), \n",
    "                    tf.identity(gen_musician_loss), \n",
    "                    tf.identity(total_cycle_loss), \n",
    "                    tf.identity(total_gen_elec_loss),\n",
    "                    tf.identity(total_gen_musician_loss),\n",
    "                    tf.identity(disc_musician_loss),\n",
    "                    tf.identity(disc_elec_loss),\n",
    "                    tf.identity(musician_iden_loss),\n",
    "                    tf.identity(elec_iden_loss)]\n",
    "\n",
    "\n",
    "    # Calculate the gradients for generator and discriminator\n",
    "    gen_elec_gradients = tape.gradient(total_gen_elec_loss, \n",
    "                                          gen_elec.trainable_variables)\n",
    "    \n",
    "    gen_musician_gradients = tape.gradient(total_gen_musician_loss, \n",
    "                                          gen_musician.trainable_variables)\n",
    "    \n",
    "    disc_musician_gradients = tape.gradient(disc_musician_loss, \n",
    "                                              disc_musician.trainable_variables)\n",
    "    disc_elec_gradients = tape.gradient(disc_elec_loss, \n",
    "                                              disc_elec.trainable_variables)\n",
    "\n",
    "    # Apply the gradients to the optimizer\n",
    "    gen_elec_opt.apply_gradients(zip(gen_elec_gradients, \n",
    "                                              gen_elec.trainable_variables))\n",
    "\n",
    "    gen_musician_opt.apply_gradients(zip(gen_musician_gradients, \n",
    "                                              gen_musician.trainable_variables))\n",
    "    \n",
    "    disc_musician_opt.apply_gradients(zip(disc_musician_gradients,\n",
    "                                                  disc_musician.trainable_variables))\n",
    "    \n",
    "    disc_elec_opt.apply_gradients(zip(disc_elec_gradients,\n",
    "                                                  disc_elec.trainable_variables))\n",
    "    \n",
    "    return all_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = np.array([[x + i*3 for x in range(3)] for i in range(1024)])\n",
    "# data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_seq_data(data, seq_l):\n",
    "    # data = np.array([[x + i*3 for x in range(3)] for i in range(1024)])\n",
    "    seq_data_list = []\n",
    "    \n",
    "    for i in range(data.shape[0] - seq_l + 1):\n",
    "        seq_data_list.append(np.expand_dims(data[i:i+seq_l], axis=0))\n",
    "    \n",
    "    seq_data = np.concatenate(seq_data_list, axis=0)\n",
    "    \n",
    "    return seq_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = to_seq_data(data=data, seq_l = 10)\n",
    "# print(a.shape)\n",
    "# print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_train_notes shape (24577, 1024, 3)\n",
      "\n",
      "notes_ds element spec TensorSpec(shape=(1024, 3), dtype=tf.float64, name=None)\n",
      "\n",
      "train_ds spec TensorSpec(shape=(None, 1024, 3), dtype=tf.float32, name=None)\n",
      "\n",
      "--------------------------------------------------\n",
      "seq_train_notes shape (24577, 1024, 3)\n",
      "\n",
      "notes_ds element spec TensorSpec(shape=(1024, 3), dtype=tf.float64, name=None)\n",
      "\n",
      "train_ds spec TensorSpec(shape=(None, 1024, 3), dtype=tf.float32, name=None)\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def notes_to_trainable_dataset(all_notes, n_data):\n",
    "    # 나중에 없애도 무방\n",
    "    \n",
    "    key_order = ['pitch', 'step', 'duration']\n",
    "    train_notes = np.stack([all_notes[key] for key in key_order], axis=1)\n",
    "    n_notes, n_features = train_notes.shape \n",
    "\n",
    "    train_notes = train_notes[:n_notes - (n_notes % SEQ_LENGTH)]\n",
    "    train_notes[:, 0] = train_notes[:, 0] / VOCAB_SIZE\n",
    "\n",
    "    seq_train_notes = to_seq_data(data=train_notes, seq_l=SEQ_LENGTH)\n",
    "    # seq_train_notes = train_notes.reshape(-1, SEQ_LENGTH, 3)\n",
    "    # seq_train_notes = seq_train_notes / np.array([VOCAB_SIZE, 0.0, 0.0])\n",
    "\n",
    "    print(f'seq_train_notes shape {seq_train_notes.shape}')\n",
    "    print()\n",
    "\n",
    "    # 여기를 다르게 바꿈 / pretrain에서 사용한 data를 사용하지 않기 위해서 :n_data -> n_data:로\n",
    "    seq_train_notes = seq_train_notes[:n_data]\n",
    "    \n",
    "    seq_ds = tf.data.Dataset.from_tensor_slices(seq_train_notes)\n",
    "    print(f'notes_ds element spec {seq_ds.element_spec}')\n",
    "    print()\n",
    "\n",
    "    # BUFFER_SIZE = n_notes - SEQ_LENGTH\n",
    "    train_ds = seq_ds.map(lambda x: (tf.cast(x, tf.float32)), num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(BATCH_SIZE, num_parallel_calls=tf.data.experimental.AUTOTUNE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    print(f'train_ds spec {train_ds.element_spec}')\n",
    "    print()\n",
    "\n",
    "    print('-' * PRINT_ITER)\n",
    "\n",
    "    return train_ds\n",
    "\n",
    "\n",
    "musician_train_ds = notes_to_trainable_dataset(data_notes_ds['musician']['train'][1], 300)\n",
    "\n",
    "elec_train_ds = notes_to_trainable_dataset(data_notes_ds['elec']['train'][1], 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function을 이용하여 code를 실행하는 도중에 .nmupy()와 같은 code 실행 가능\n",
    "# https://github.com/tgjeon/TF-Eager-Execution-Guide-KR/blob/master/guide.md 참고\n",
    "# https://www.tensorflow.org/api_docs/python/tf/config/run_functions_eagerly\n",
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 36\n",
    "VERSION = '19'\n",
    "SAVE_PATH = 'runs_test5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter(f'{SAVE_PATH}/v{VERSION}-epoch{EPOCHS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# problem\n",
    "1. nan 값 -> https://data-newbie.tistory.com/281\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_cnt = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. learning rate가 커서 gradient discent 과정에서 튕겨져 나감\n",
    "2. learning rate의 크기 차이로 gen과 disc의 학습에 충분한 시간이 부여되지 않음\n",
    "\n",
    "-> learning rate에 대해서 gen, disc 각각 작게, 크게 했을때 변화하는 값 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6011 (pid 2023924), started 0:00:39 ago. (Use '!kill 2023924' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-34d8d9b17435c201\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-34d8d9b17435c201\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6011;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    }
   ],
   "source": [
    "# tensorboard graph color rgb : #1fddff\n",
    "# disc -> 틀렸을때, 강한 피드백(scalar)을 작용 -> 학습에 오랜 시간\n",
    "losses_ratio = {'disc' : 1.0, 'gen' : 1.0, 'cyc' : 1.0, 'iden' : 1.0} # cycle, iden loss ratio는 서로 종속적임(in tf document)\n",
    "all_loss = []\n",
    "all_loss_name = ['gen_elec_loss', \n",
    "                 'gen_musician_loss',\n",
    "                 'total_cycle_loss', \n",
    "                 'total_gen_elec_loss',\n",
    "                 'total_gen_musician_loss',\n",
    "                 'disc_musician_loss',\n",
    "                 'disc_elec_loss',\n",
    "                 'musician_iden_loss',\n",
    "                 'elec_iden_loss']\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {SAVE_PATH} --host 0.0.0.0 --port=6011\n",
    "\n",
    "# 단순히 모델의 가중치 업데이트 횟수를 의미함\n",
    "# 향후 이 값을 바탕으로 epoch를 계산 가능 | \n",
    "# update_n = epoch * (n_notes / (seq_length * batch_size)) = epoch * (n_seq_notes / batch_size)\n",
    "\n",
    "# update_cnt = 0\n",
    "\n",
    "total_start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    n = 0\n",
    "    \n",
    "    for musician, elec in tf.data.Dataset.zip((musician_train_ds, elec_train_ds)):\n",
    "        # musician = tf.cast(musician, dtype=tf.float32)\n",
    "        # elec = tf.cast(elec, dtype=tf.float32)\n",
    "                \n",
    "        all_loss = [0 for _ in range(len(all_loss_name))]   \n",
    "\n",
    "        for_all_loss = train_step(musician, elec, losses_ratio)\n",
    "        for i, loss in enumerate(for_all_loss):\n",
    "            all_loss[i] += loss.numpy().item()\n",
    "\n",
    "        if update_cnt % 100 == 0:\n",
    "            print('-', end='')\n",
    "#         if update_cnt % 4500 == 0 and update_cnt != 0:\n",
    "#             gen_musician.save(f'model_save/v{VERSION}/gen_musician-epoch{EPOCHS}-step4500.h5')\n",
    "#             gen_elec.save(f'model_save/v{VERSION}/gen_elec-epoch{EPOCHS}-step4500.h5')\n",
    "#             disc_musician.save(f'model_save/v{VERSION}/disc_musician-epoch{EPOCHS}-step4500.h5')\n",
    "#             disc_elec.save(f'model_save/v{VERSION}/disc_elec-epoch{EPOCHS}.h5')\n",
    "        \n",
    "        n += 1\n",
    "        update_cnt += 1\n",
    "\n",
    "        # 아래 부분에서 BATCH_SIZE로 나눌 지에 관해 고민 필요\n",
    "        all_loss = [loss for loss in all_loss]\n",
    "        for loss, loss_name in zip(all_loss, all_loss_name):\n",
    "            writer.add_scalars('check_info/', {loss_name : loss}, update_cnt)\n",
    "\n",
    "    # clear_output(wait=True)\n",
    "    \n",
    "    # note를 시각화할 수 있는 거 넣기\n",
    "    # ex) generate_images(generator_g, sample_horse)\n",
    "    \n",
    "#     if (epoch + 1) % 3 == 0:\n",
    "#         ckpt_save_path = ckpt_manager.save()\n",
    "#         print(f'Saving checkpoint for each {epoch + 1} at {ckpt_save_path}')\n",
    "    \n",
    "    print(f'Time taken for epoch {epoch + 1} is {time.time() - start:.3f} sec\\n')\n",
    "\n",
    "print()\n",
    "print('-' * PRINT_ITER)\n",
    "print(f'Time taken for total {EPOCHS} epoch : {time.time() - total_start_time:.3f} sec')\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_domain_notes(input_ds, model):\n",
    "    result_list = []\n",
    "    generated_notes_list = []\n",
    "    n_batch_data = tf.data.experimental.cardinality(input_ds).numpy().item()\n",
    "    # n_batch_data = 1\n",
    "\n",
    "    for inp in input_ds.take(n_batch_data):\n",
    "        gen_music = model(inp)\n",
    "        result_list.append(gen_music)\n",
    "\n",
    "    for result in result_list:\n",
    "        pitchs = result[:, :, 0]\n",
    "        steps = result[:, :, 1]\n",
    "        durations = result[:, :, 2]\n",
    "\n",
    "        pitchs = tf.reshape(pitchs, [-1])\n",
    "        pitchs = tf.math.scalar_mul(VOCAB_SIZE, pitchs)\n",
    "        pitchs = tf.math.round(pitchs)\n",
    "        pitchs = tf.cast(pitchs, dtype=tf.int32)\n",
    "        pitchs = pitchs.numpy()\n",
    "\n",
    "        durations = tf.reshape(durations, [-1])\n",
    "        durations = durations.numpy()\n",
    "\n",
    "        steps = tf.reshape(steps, [-1])\n",
    "        steps = steps.numpy() \n",
    "\n",
    "        generated_notes = []\n",
    "        prev_start = 0\n",
    "\n",
    "        for pitch, duration, step in zip(pitchs, durations, steps):\n",
    "            pitch, duration, step = pitch.item(), duration.item(), step.item()\n",
    "\n",
    "            if pitch > VOCAB_SIZE - 1:\n",
    "                pitch = VOCAB_SIZE - 1\n",
    "            if pitch < 0:\n",
    "                pitch = 0\n",
    "                \n",
    "            start = prev_start + step \n",
    "            end = start + duration\n",
    "            input_note = (pitch, step, duration)\n",
    "            generated_notes.append((*input_note, start, end))\n",
    "\n",
    "            prev_start = start\n",
    "\n",
    "        generated_notes = pd.DataFrame(\n",
    "            generated_notes, columns=(*key_order, 'start', 'end')\n",
    "        )\n",
    "\n",
    "        generated_notes_list.append(generated_notes)\n",
    "\n",
    "    generated_notes_all = pd.concat(generated_notes_list, ignore_index=True)\n",
    "    return generated_notes_all\n",
    "\n",
    "def notes_to_testable_dataset(all_notes):\n",
    "    key_order = ['pitch', 'step', 'duration']\n",
    "    test_notes = np.stack([all_notes[key] for key in key_order], axis=1)\n",
    "    n_notes, n_features = test_notes.shape \n",
    "    \n",
    "    n_notes_for_seq = n_notes - (n_notes % SEQ_LENGTH)\n",
    "    test_notes = test_notes[:n_notes_for_seq]\n",
    "    \n",
    "    test_notes = test_notes.reshape(-1, SEQ_LENGTH, 3)\n",
    "    print(f'test_notes shape {test_notes.shape}')\n",
    "    print()\n",
    "    \n",
    "    notes_ds = tf.data.Dataset.from_tensor_slices(test_notes)\n",
    "    print(f'notes_ds element spec {notes_ds.element_spec}')\n",
    "    print()\n",
    "\n",
    "    seq_ds = notes_ds\n",
    "    print(f'seq_ds element_spec {seq_ds.element_spec}')\n",
    "    print()\n",
    "\n",
    "    # BUFFER_SIZE = n_notes - SEQ_LENGTH\n",
    "    test_ds = seq_ds.batch(n_notes_for_seq, num_parallel_calls=tf.data.experimental.AUTOTUNE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    print(f'test_ds spec {test_ds.element_spec}')\n",
    "    print()\n",
    "\n",
    "    print('-' * PRINT_ITER)\n",
    "\n",
    "    return test_ds\n",
    "\n",
    "def generate_notes(midi_filepath, model):\n",
    "    notes = midi_to_notes(midi_filepath)\n",
    "    test_ds = notes_to_testable_dataset(notes)\n",
    "    generated_notes = generate_domain_notes(test_ds, model)\n",
    "    return generated_notes\n",
    "    \n",
    "\n",
    "midi_filepath = elec_filenames[100]\n",
    "\n",
    "generated_notes = generate_notes(midi_filepath, gen_musician)\n",
    "print(generated_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_filepath = elec_filenames[666]\n",
    "\n",
    "generated_notes = generate_notes(midi_filepath, gen_musician)\n",
    "generated_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_notes = midi_to_notes(midi_filepath)\n",
    "real_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sqrt(np.sum(np.abs((generated_notes['pitch'] - real_notes['pitch'][len(generated_notes)]).to_numpy())) / (len(generated_notes) / (SEQ_LENGTH*BATCH_SIZE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = f'model_save/v{VERSION}/test.mid'\n",
    "out_pm = notes_to_midi(\n",
    "    generated_notes, out_file=test_file, instrument_name=instrument_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = f'model_save/v{VERSION}/output_gen_musician-epoch{EPOCHS}.mid'\n",
    "out_pm = notes_to_midi(\n",
    "    generated_notes, out_file=out_file, instrument_name=instrument_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = f'model_save/v{VERSION}/output_real_elec-epoch{EPOCHS}.mid'\n",
    "out_pm = notes_to_midi(\n",
    "    real_notes, out_file=out_file, instrument_name=instrument_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_musician.save(f'model_save/v{VERSION}/gen_musician-epoch{EPOCHS}.h5')\n",
    "gen_elec.save(f'model_save/v{VERSION}/gen_elec-epoch{EPOCHS}.h5')\n",
    "disc_musician.save(f'model_save/v{VERSION}/disc_musician-epoch{EPOCHS}.h5')\n",
    "disc_elec.save(f'model_save/v{VERSION}/disc_elec-epoch{EPOCHS}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generator 불러오기\n",
    "# gen_musician_ckpt_path = ''\n",
    "# gen_musician.load_weights(gen_musician_ckpt_path)\n",
    "\n",
    "# gen_elec_ckpt_path = '' \n",
    "# gen_elec.load_weights(gen_elec_ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
