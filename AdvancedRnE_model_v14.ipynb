{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPnX8kayLjGC"
      },
      "source": [
        "# Version Info\n",
        "v5 version에서 cycleGAN 부분을 아래 notebook을 바탕으로 수정 중 == v6 version<br>https://colab.research.google.com/drive/1n5OzIaic1Ho60UAtqEgCnRyn7Lg1Po_v<br>sequence 길이를 모두 1로 맞춰야 함, 그렇지 않으면 생성할 때 마다 차원이 감소함. 아니면 train 과정에서 모델이 seq 길이 만큼 하나의 데이터에 대해 반복을 수행해서 생성을 진행해야 한다.\n",
        "\n",
        "v6 version lstm cell return_sequnces=True로 설정하고 마지막 output layer에서 2차원 dense를 수행<br> --> seq_l을 데이터의 차원으로 갖도록 함\n",
        "\n",
        "cycleGAN의 generation 부분 -> v7\n",
        "\n",
        "cycleGAN의 loss 및 훈련 -> v8\n",
        "\n",
        "gradients tape 문제 해결을 위한 tf dataset 이용 -> v9\n",
        "\n",
        "v9의 cycleGAN pt type code 정리 및 파이프라인 보수 -> v10<br>\n",
        "v10에서 seq화 과정에서 발생하는 데이터 손실, loss 그래프, gpu 사용량 최적화, notes to MIDI(생성 데이터 MIDI로 변환) 수정 -> v11<br>\n",
        "v11에서 todo 6~ 까지의 수정 -> v12\n",
        "v12에서 vocab size 관련 error 수정 + batch 등의 파라미터 조정 -> v13\n",
        "v13에서 만든 pt type 완성본을 여러가지 방면으로 실험 -> v14"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoiMShYhCSSw"
      },
      "source": [
        "# TODO\n",
        "1. 일반 LSTM model과는 다르게 cycleGAN의 input data를 만들어야 한다.\n",
        "즉, sequence data로 data를 변환할 때, 일반 LSTM model과는 달리 target data를 생성하지 않아야 하며 seq_L 만큼 마다 나눠서 데이터 손실 없이 input data로 사용해야한다.\n",
        "\n",
        "    -> 모델이 seq_l 길이 만큼의 데이터를 바탕으로 (그 길이 안 데이터 해석) output을 만들어 낸다.<br> 이후 sequence data로 나뉘어진 output data를 다시 reshape한 후, midi로 변환하면 된다.\n",
        "\n",
        "2. sequence data 및 음악적 특성을 충분히 고려할 수 있도록 discriminator 및 그에 따른 loss function 구현 필요 -- 시간 계획 필수 (기한 고려)\n",
        "\n",
        "3. 향후 cycleGAN과 관련된 loss(cycle consistnecy 등)도 손을 볼 수 있으면 좋을 것 같다.\n",
        "\n",
        "4. GPU 사용량 늘리기\n",
        "\n",
        "5. batch를 만들때, shuffle 주의하기, test data에는 사용하지 않아야 할수도 있다. -> train data shuffle을 수행해도 무방할 정도의 seq_l을 잡거나 shuffle을 수행하지 않아야 한다.\n",
        "\n",
        "6. reshape 이용해서 create sequence 부분 고치기\n",
        "\n",
        "7. 실제 연구 데이터를 사용할 때, 두 도메인의 데이터 길이 조심하기 - 학습과정에서\n",
        "\n",
        "8. notes_to_midi 잘 이용하기\n",
        "\n",
        "9. MIDI file 제대로, 전부 append 되는 지 확인하기 --> file_path glob 부분에서 slicing에 의한 문제 --> 해결\n",
        "\n",
        "10. cycleGAN 생성 과정 중에 pitch, step, duration의 loss 차이가 튼 영향을 미친다면 music_generation 노트북을 활용하여 loss 재정의하기\n",
        "\n",
        "11. 각 노래 midi 별로 나눠서 처리하거나, batch size 크기를 늘려 모델이 batch로 학습을 진행할 때, 각각의 노래가 섞이는 경우를 조금은 줄이는 것이 좋은지 실험 필요\n",
        "\n",
        "12. colab 환경에서 여러개의 notebook을 띄워 놓은 후, 각각의 변수들을 변경하면서 실험 진행<br>\n",
        "\n",
        "13. 12를 진행하는 과정에서 실험 리스트, 설계 과정, 결과 등을 모두 기록해야함\n",
        "\n",
        "14. colab notebook 최대 가용량 -> 실험 가능량 설계 -> 시간 계획\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLJm-R6HbtFC"
      },
      "source": [
        "# Experiment plan\n",
        "1. batch\n",
        "2. data<br>\n",
        "    1. count\n",
        "    2. type<br>\n",
        "        1. musician<br>\n",
        "            1. piano-e-competition\n",
        "        2. elec<br>\n",
        "            1. maestro\n",
        "3. generator model<br>\n",
        "    1. model structure<br>\n",
        "        1. type<br>\n",
        "            1. bidirectional\n",
        "            2. Dense\n",
        "            3. GRU\n",
        "        2. layer<br>\n",
        "            1. unit count\n",
        "    2. hyper parameter\n",
        "4. discriminator model\n",
        "    1. model structure<br>\n",
        "        1. type<br>\n",
        "            1. bidirectional\n",
        "            2. Dense\n",
        "            3. GRU\n",
        "        2. layer<br>\n",
        "            1. unit count\n",
        "    2. hyper parameter<br>\n",
        "    3. for music<br>\n",
        "        1. patchGAN\n",
        "5. loss function weight<br>\n",
        "    1. brute force method\n",
        "6. optimizer<br>\n",
        "    1. type\n",
        "    2. learning rate\n",
        "7. future plan<br>\n",
        "    1. 특정곡의 곡 또는 작곡관련 nlp 정보 -> embbding -> 새로운 층으로 concatenate -> multimodal<br>\n",
        "    2. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDxtxjFDDhdz",
        "outputId": "9e3a9398-253b-4bfa-af97-9328a9843ad2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mTraceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip\", line 5, in <module>\n",
            "    from pip._internal.cli.main import main\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/main.py\", line 9, in <module>\n",
            "    from pip._internal.cli.autocompletion import autocomplete\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/autocompletion.py\", line 10, in <module>\n",
            "    from pip._internal.cli.main_parser import create_main_parser\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/main_parser.py\", line 9, in <module>\n",
            "    from pip._internal.build_env import get_runnable_pip\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/build_env.py\", line 20, in <module>\n",
            "    from pip._internal.cli.spinners import open_spinner\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/spinners.py\", line 9, in <module>\n",
            "    from pip._internal.utils.logging import get_indentation\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/utils/logging.py\", line 8, in <module>\n",
            "    from dataclasses import dataclass\n",
            "  File \"/usr/lib/python3.7/dataclasses.py\", line 200, in <module>\n",
            "    _MODULE_IDENTIFIER_RE = re.compile(r'^(?:\\s*(\\w+)\\s*\\.)?\\s*(\\w+)')\n",
            "  File \"/usr/lib/python3.7/re.py\", line 236, in compile\n",
            "    return _compile(pattern, flags)\n",
            "  File \"/usr/lib/python3.7/re.py\", line 288, in _compile\n",
            "    p = sre_compile.compile(pattern, flags)\n",
            "  File \"/usr/lib/python3.7/sre_compile.py\", line 764, in compile\n",
            "    p = sre_parse.parse(p, flags)\n",
            "  File \"/usr/lib/python3.7/sre_parse.py\", line 924, in parse\n",
            "    p = _parse_sub(source, pattern, flags & SRE_FLAG_VERBOSE, 0)\n",
            "  File \"/usr/lib/python3.7/sre_parse.py\", line 420, in _parse_sub\n",
            "    not nested and not items))\n",
            "  File \"/usr/lib/python3.7/sre_parse.py\", line 810, in _parse\n",
            "    p = _parse_sub(source, state, sub_verbose, nested + 1)\n",
            "  File \"/usr/lib/python3.7/sre_parse.py\", line 420, in _parse_sub\n",
            "    not nested and not items))\n",
            "  File \"/usr/lib/python3.7/sre_parse.py\", line 640, in _parse\n",
            "    item = subpattern[-1:]\n",
            "  File \"/usr/lib/python3.7/sre_parse.py\", line 166, in __getitem__\n",
            "    return SubPattern(self.pattern, self.data[index])\n",
            "  File \"/usr/lib/python3.7/sre_parse.py\", line 111, in __init__\n",
            "    def __init__(self, pattern, data=None):\n",
            "KeyboardInterrupt\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# for installing\n",
        "\n",
        "!pip install -U -q pip\n",
        "!pip install -q tensorboardX\n",
        "!pip install -q git+https://github.com/tensorflow/examples.git\n",
        "!pip install -q pretty_midi\n",
        "!pip install -q knockknock"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Vl3Sf8IYi2e",
        "outputId": "97f3cb30-5092-4047-fa24-806a2c7b232e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning into 'piano-e-competition-midi-files'...\n",
            "Updating files:   4% (241/5486)\n",
            "Updating files:   5% (275/5486)\n",
            "Updating files:   6% (330/5486)\n",
            "Updating files:   7% (385/5486)\n",
            "Updating files:   8% (439/5486)\n",
            "Updating files:   9% (494/5486)\n",
            "Updating files:  10% (549/5486)\n",
            "Updating files:  10% (570/5486)\n",
            "Updating files:  11% (604/5486)\n",
            "Updating files:  12% (659/5486)\n",
            "Updating files:  13% (714/5486)\n",
            "Updating files:  14% (769/5486)\n",
            "Updating files:  15% (823/5486)\n",
            "Updating files:  15% (864/5486)\n",
            "Updating files:  16% (878/5486)\n",
            "Updating files:  17% (933/5486)\n",
            "Updating files:  18% (988/5486)\n",
            "Updating files:  19% (1043/5486)\n",
            "Updating files:  20% (1098/5486)\n",
            "Updating files:  21% (1153/5486)\n",
            "Updating files:  21% (1197/5486)\n",
            "Updating files:  22% (1207/5486)\n",
            "Updating files:  23% (1262/5486)\n",
            "Updating files:  24% (1317/5486)\n",
            "Updating files:  25% (1372/5486)\n",
            "Updating files:  25% (1388/5486)\n",
            "Updating files:  26% (1427/5486)\n",
            "Updating files:  27% (1482/5486)\n",
            "Updating files:  28% (1537/5486)\n",
            "Updating files:  29% (1591/5486)\n",
            "Updating files:  30% (1646/5486)\n",
            "Updating files:  31% (1701/5486)\n",
            "Updating files:  32% (1756/5486)\n",
            "Updating files:  32% (1775/5486)\n",
            "Updating files:  33% (1811/5486)\n",
            "Updating files:  34% (1866/5486)\n",
            "Updating files:  35% (1921/5486)\n",
            "Updating files:  36% (1975/5486)\n",
            "Updating files:  36% (2003/5486)\n",
            "Updating files:  37% (2030/5486)\n",
            "Updating files:  38% (2085/5486)\n",
            "Updating files:  38% (2126/5486)\n",
            "Updating files:  39% (2140/5486)\n",
            "Updating files:  40% (2195/5486)\n",
            "Updating files:  41% (2250/5486)\n",
            "Updating files:  41% (2275/5486)\n",
            "Updating files:  42% (2305/5486)\n",
            "Updating files:  43% (2359/5486)\n",
            "Updating files:  44% (2414/5486)\n",
            "Updating files:  45% (2469/5486)\n",
            "Updating files:  45% (2498/5486)\n",
            "Updating files:  46% (2524/5486)\n",
            "Updating files:  47% (2579/5486)\n",
            "Updating files:  48% (2634/5486)\n",
            "Updating files:  49% (2689/5486)\n",
            "Updating files:  49% (2701/5486)\n",
            "Updating files:  50% (2743/5486)\n",
            "Updating files:  51% (2798/5486)\n",
            "Updating files:  52% (2853/5486)\n",
            "Updating files:  52% (2866/5486)\n",
            "Updating files:  53% (2908/5486)\n",
            "Updating files:  54% (2963/5486)\n",
            "Updating files:  55% (3018/5486)\n",
            "Updating files:  56% (3073/5486)\n",
            "Updating files:  56% (3112/5486)\n",
            "Updating files:  57% (3128/5486)\n",
            "Updating files:  58% (3182/5486)\n",
            "Updating files:  59% (3237/5486)\n",
            "Updating files:  60% (3292/5486)\n",
            "Updating files:  61% (3347/5486)\n",
            "Updating files:  61% (3374/5486)\n",
            "Updating files:  62% (3402/5486)\n",
            "Updating files:  63% (3457/5486)\n",
            "Updating files:  64% (3512/5486)\n",
            "Updating files:  65% (3566/5486)\n",
            "Updating files:  66% (3621/5486)\n",
            "Updating files:  67% (3676/5486)\n",
            "Updating files:  67% (3698/5486)\n",
            "Updating files:  68% (3731/5486)\n",
            "Updating files:  69% (3786/5486)\n",
            "Updating files:  70% (3841/5486)\n",
            "Updating files:  71% (3896/5486)\n",
            "Updating files:  72% (3950/5486)\n",
            "Updating files:  72% (3960/5486)\n",
            "Updating files:  73% (4005/5486)\n",
            "Updating files:  74% (4060/5486)\n",
            "Updating files:  75% (4115/5486)\n",
            "Updating files:  75% (4133/5486)\n",
            "Updating files:  76% (4170/5486)\n",
            "Updating files:  77% (4225/5486)\n",
            "Updating files:  78% (4280/5486)\n",
            "Updating files:  79% (4334/5486)\n",
            "Updating files:  80% (4389/5486)\n",
            "Updating files:  80% (4429/5486)\n",
            "Updating files:  81% (4444/5486)\n",
            "Updating files:  82% (4499/5486)\n",
            "Updating files:  83% (4554/5486)\n",
            "Updating files:  84% (4609/5486)\n",
            "Updating files:  85% (4664/5486)\n",
            "Updating files:  85% (4707/5486)\n",
            "Updating files:  86% (4718/5486)\n",
            "Updating files:  87% (4773/5486)\n",
            "Updating files:  88% (4828/5486)\n",
            "Updating files:  89% (4883/5486)\n",
            "Updating files:  90% (4938/5486)\n",
            "Updating files:  91% (4993/5486)\n",
            "Updating files:  91% (5021/5486)\n",
            "Updating files:  92% (5048/5486)\n",
            "Updating files:  93% (5102/5486)\n",
            "Updating files:  94% (5157/5486)\n",
            "Updating files:  95% (5212/5486)\n",
            "Updating files:  96% (5267/5486)\n",
            "Updating files:  96% (5310/5486)\n",
            "Updating files:  97% (5322/5486)\n",
            "Updating files:  98% (5377/5486)\n",
            "Updating files:  99% (5432/5486)\n",
            "Updating files: 100% (5486/5486)\n",
            "Updating files: 100% (5486/5486), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/chelate1118/piano-e-competition-midi-files.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkkfwClODhd4",
        "outputId": "37f57342-1059-4d14-d4ab-63b102e352e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\gobak\\.virtualenvs\\.venv-HOEqjjdC\\Scripts\\python.exe\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "print(sys.executable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zXcyh1z9Dhd4"
      },
      "outputs": [],
      "source": [
        "# import os \n",
        "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Qle2WAaUkKYp"
      },
      "outputs": [],
      "source": [
        "# import 과정 error -> AVX2를 이용한 tf compile 진행 시, 더 최적화 가능함을 알려줌 | 아래 링크 참조\n",
        "# http://erpcomputing.com/build-tensorflow-from-source--mkl-enabled\n",
        "\n",
        "#library import\n",
        "\n",
        "# for data and preprocessing\n",
        "import tensorflow_datasets as tfds\n",
        "import collections\n",
        "import glob\n",
        "import numpy as np\n",
        "import pathlib\n",
        "import pandas as pd\n",
        "import pretty_midi\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "# import MIDI_Pedal # 사용법 : MIDI_Pedal.apply('test.mid', 'output.mid')\n",
        "\n",
        "\n",
        "# for model\n",
        "# 설치가 git인 경우 local에서 설치한 후 .venv/Lib에 넣으면 됨, local pip 설치경로는 위에 있음\n",
        "import tensorflow as tf\n",
        "\n",
        "# for the others\n",
        "# import fluidsynth # MIDI는 다른 거로 재생하기\n",
        "import datetime\n",
        "from IPython import display\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Dict, List, Optional, Sequence, Tuple\n",
        "import time\n",
        "import os\n",
        "# from knockknock import desktop_sender\n",
        "# from google.colab import files # google colab에서 결과 midi file 다운받고 싶을 때 import 하면 됨."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlXj0TQDlBvY",
        "outputId": "59dce5b9-e707-41e2-abff-c2c6861e34b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of files: 1282\n",
            "data\\maestro-v2.0.0\\2004\\MIDI-Unprocessed_SMF_02_R1_2004_01-05_ORIG_MID--AUDIO_02_R1_2004_05_Track05_wav.midi\n"
          ]
        }
      ],
      "source": [
        "# 단순 model test용 데이터\n",
        "\n",
        "data_dir = pathlib.Path('data/maestro-v2.0.0')\n",
        "if not data_dir.exists():\n",
        "  tf.keras.utils.get_file(\n",
        "      'maestro-v2.0.0-midi.zip',\n",
        "      origin='https://storage.googleapis.com/magentadata/datasets/maestro/v2.0.0/maestro-v2.0.0-midi.zip',\n",
        "      extract=True,\n",
        "      cache_dir='.', cache_subdir='data',\n",
        "  )\n",
        "\n",
        "filenames = glob.glob(str(data_dir/'**/*.mid*'))\n",
        "print('Number of files:', len(filenames))\n",
        "\n",
        "sample_file = filenames[0]\n",
        "print(sample_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asG7Fu5AEcsN",
        "outputId": "5ae811af-96ab-4948-c5ff-52defc5731d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of instruments: 1\n",
            "Instrument name: Acoustic Grand Piano\n"
          ]
        }
      ],
      "source": [
        "pm = pretty_midi.PrettyMIDI(sample_file)\n",
        "\n",
        "print('Number of instruments:', len(pm.instruments))\n",
        "instrument = pm.instruments[0]\n",
        "instrument_name = pretty_midi.program_to_instrument_name(instrument.program)\n",
        "print('Instrument name:', instrument_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JBrfs9B2kKYq"
      },
      "outputs": [],
      "source": [
        "# global variable declaration\n",
        "\n",
        "#global\n",
        "seed = 42\n",
        "\n",
        "# for model\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "tf.random.set_seed(seed)\n",
        "BUFFER_SIZE = -1\n",
        "BATCH_SIZE = 1 # cycleGAN tf document 참고한 값 \n",
        "SEQ_LENGTH = 512\n",
        "VOCAB_SIZE = 128\n",
        "\n",
        "N_FEATURE = OUTPUT_CHANNELS = 3\n",
        "LR = 2e-4 # opt\n",
        "B1 = 0.5 # opt\n",
        "\n",
        "# for data\n",
        "np.random.seed(seed)\n",
        "# MUSICIAN_MIDI_FILES_DIR = pathlib.Path('data/maestro-v2.0.0')\n",
        "MUSICIAN_MIDI_FILES_DIR = pathlib.Path('piano-e-competition-midi-files/midi')\n",
        "ELEC_MIDI_FILES_DIR = pathlib.Path('data/maestro-v2.0.0')\n",
        "\n",
        "# for the others\n",
        "# Sampling rate for audio playback\n",
        "_SAMPLING_RATE = 16000\n",
        "PRINT_ITER = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "N_FEATURE = OUTPUT_CHANNELS = 3\n",
        "LR = 2e-4 # opt\n",
        "B1 = 0.5 # opt\n",
        "\n",
        "# for data\n",
        "np.random.seed(seed)\n",
        "# MUSICIAN_MIDI_FILES_DIR = pathlib.Path('data/maestro-v2.0.0')\n",
        "MUSICIAN_MIDI_FILES_DIR = pathlib.Path('piano-e-competition-midi-files/midi')\n",
        "ELEC_MIDI_FILES_DIR = pathlib.Path('data/maestro-v2.0.0')\n",
        "\n",
        "# for the others\n",
        "# Sampling rate for audio playback\n",
        "_SAMPLING_RATE = 16000\n",
        "PRINT_ITER = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9vQFy3HnGq4P"
      },
      "outputs": [],
      "source": [
        "musician_filenames = glob.glob(str(MUSICIAN_MIDI_FILES_DIR/'*.mid*'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mOYEoJpkKYr"
      },
      "source": [
        "용어 정리<br>real : 실제 연주자 연주<br>elec : 전자 음악 연주"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8EZUtq-Sljod"
      },
      "outputs": [],
      "source": [
        "def midi_to_notes(midi_file: str) -> pd.DataFrame:\n",
        "  pm = pretty_midi.PrettyMIDI(midi_file)\n",
        "  instrument = pm.instruments[0]\n",
        "  notes = collections.defaultdict(list)\n",
        "\n",
        "  # Sort the notes by start time\n",
        "  sorted_notes = sorted(instrument.notes, key=lambda note: note.start)\n",
        "  prev_start = sorted_notes[0].start\n",
        "\n",
        "  for note in sorted_notes:\n",
        "    start = note.start\n",
        "    end = note.end\n",
        "    notes['pitch'].append(note.pitch)\n",
        "    notes['start'].append(start)\n",
        "    notes['end'].append(end)\n",
        "    notes['step'].append(start - prev_start)\n",
        "    notes['duration'].append(end - start)\n",
        "    prev_start = start\n",
        "\n",
        "  return pd.DataFrame({name: np.array(value) for name, value in notes.items()})\n",
        "\n",
        "def plot_piano_roll(notes: pd.DataFrame, count: Optional[int] = None):\n",
        "    if count:\n",
        "      title = f'First {count} notes'\n",
        "    else:\n",
        "      title = f'Whole track'\n",
        "      count = len(notes['pitch'])\n",
        "    plt.figure(figsize=(20, 4))\n",
        "    plot_pitch = np.stack([notes['pitch'], notes['pitch']], axis=0)\n",
        "    plot_start_stop = np.stack([notes['start'], notes['end']], axis=0)\n",
        "    plt.plot(\n",
        "        plot_start_stop[:, :count], plot_pitch[:, :count], color=\"b\", marker=\".\")\n",
        "    plt.xlabel('Time [s]')\n",
        "    plt.ylabel('Pitch')\n",
        "    _ = plt.title(title)\n",
        "\n",
        "def plot_distributions(notes: pd.DataFrame, drop_percentile=2.5):\n",
        "  plt.figure(figsize=[15, 5])\n",
        "  plt.subplot(1, 3, 1)\n",
        "  sns.histplot(notes, x=\"pitch\", bins=20)\n",
        "\n",
        "  plt.subplot(1, 3, 2)\n",
        "  max_step = np.percentile(notes['step'], 100 - drop_percentile)\n",
        "  sns.histplot(notes, x=\"step\", bins=np.linspace(0, max_step, 21))\n",
        "  \n",
        "  plt.subplot(1, 3, 3)\n",
        "  max_duration = np.percentile(notes['duration'], 100 - drop_percentile)\n",
        "  sns.histplot(notes, x=\"duration\", bins=np.linspace(0, max_duration, 21))\n",
        "\n",
        "def notes_to_midi(\n",
        "  notes: pd.DataFrame,\n",
        "  out_file: str, \n",
        "  instrument_name: str,\n",
        "  velocity: int = 100,  # note loudness\n",
        ") -> pretty_midi.PrettyMIDI:\n",
        "\n",
        "  pm = pretty_midi.PrettyMIDI()\n",
        "  instrument = pretty_midi.Instrument(\n",
        "      program=pretty_midi.instrument_name_to_program(\n",
        "          instrument_name))\n",
        "\n",
        "  prev_start = 0\n",
        "  for i, note in notes.iterrows():\n",
        "    start = float(prev_start + note['step'])\n",
        "    end = float(start + note['duration'])\n",
        "    note = pretty_midi.Note(\n",
        "        velocity=velocity,\n",
        "        pitch=int(note['pitch']),\n",
        "        start=start,\n",
        "        end=end,\n",
        "    )\n",
        "    instrument.notes.append(note)\n",
        "    prev_start = start\n",
        "\n",
        "  pm.instruments.append(instrument)\n",
        "  pm.write(out_file)\n",
        "  return pm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2N7zHJrkKYs"
      },
      "source": [
        "##### 참고\n",
        "<span style='font-size:80%'>1. midi_to_notes(midi_file: str) -> pd.DataFrame<br>2. notes_to_midi(\n",
        "  notes: pd.DataFrame,\n",
        "  out_file: str, \n",
        "  instrument_name: str,\n",
        "  velocity: int = 100,  # note loudness\n",
        ") -> pretty_midi.PrettyMIDI<br>3. plot_piano_roll(notes: pd.DataFrame, count: Optional[int] = None)<br>4. def plot_distributions(notes: pd.DataFrame, drop_percentile=2.5)</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRWkDaY5DheA",
        "outputId": "394ded22-d396-4edf-8454-76bc1c89836e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1282"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "musician_filenames = glob.glob(str(ELEC_MIDI_FILES_DIR/'**/*.mid*'))\n",
        "\n",
        "len(musician_filenames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8u_hAXuLkKYz",
        "outputId": "331acb6f-79d1-4d47-9316-196f36c4f317"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "process 99.950% 4010th file process continue...\n",
            "\n",
            "End!\n",
            "total process time for 4012 files : 2576.241sec\n"
          ]
        }
      ],
      "source": [
        "# data import musician AND FAKE\n",
        "# error 많이 나면 np.array 이용하기\n",
        "# filenames slicing 수 적절히 설정하기\n",
        "\n",
        "# musician_filenames = glob.glob(str(MUSICIAN_MIDI_FILES_DIR/'**/*.mid*'))\n",
        "musician_filenames = glob.glob(str(MUSICIAN_MIDI_FILES_DIR/'*.mid*'))\n",
        "elec_filenames = glob.glob(str(ELEC_MIDI_FILES_DIR/'**/*.mid*'))\n",
        "\n",
        "\n",
        "musician_all_notes = []\n",
        "elec_all_notes = []\n",
        "\n",
        "# 각 notes listz\n",
        "# 0 : musician - 사람이 연주한 데이터\n",
        "# 1 : fake - 사람이 연주하지 않은 데이터\n",
        "data_notes = [musician_all_notes, elec_all_notes]\n",
        "data_paths = [musician_filenames, elec_filenames]\n",
        "data_notes_ds = {} # tfds를 이용한 slices 이후 데이터임, (n_notes, tf_dt) tuple형의 데이터가 원소인 list\n",
        "\n",
        "train_data_notes_ds = [] # n_notes, dataset(slice)의 tuple 형태\n",
        "validation_data_notes_ds = []\n",
        "test_data_notes_ds = []\n",
        "\n",
        "data_split_ratio = {'train' : 0.8, 'validation' : 0.2, 'test' : 0.2}\n",
        "\n",
        "key_order = ['pitch', 'step', 'duration']\n",
        "not_key_order = ['start', 'end']\n",
        "\n",
        "data_type = ['musician', 'elec']\n",
        "\n",
        "# notes 수를 맞추기 위함\n",
        "n_notes = 10e9\n",
        "\n",
        "file_cnt = 0\n",
        "\n",
        "print(f'{len(musician_filenames) + len(elec_filenames)} files are ready to process')\n",
        "print('-' * PRINT_ITER)\n",
        "print()\n",
        "print('Start!')\n",
        "print()\n",
        "time.sleep(2)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for i, data_path in enumerate(data_paths):\n",
        "    for f in data_path:\n",
        "        notes = midi_to_notes(f)\n",
        "        data_notes[i].append(notes)\n",
        "        if file_cnt % 10 == 0:\n",
        "            clear_output(wait=True)\n",
        "            print(f'process {(file_cnt / (len(musician_filenames) + len(elec_filenames))) * 100.0:.3f}% {file_cnt}th file process continue...')\n",
        "        file_cnt += 1\n",
        "        \n",
        "    # append와 concat은 다르다!\n",
        "    data_notes[i] = pd.concat(data_notes[i])\n",
        "    data_notes[i].drop(columns=not_key_order, inplace=True)\n",
        "    n_notes = min(len(data_notes[i]), n_notes)    \n",
        "    \n",
        "for i, data_path in enumerate(data_paths):\n",
        "    _train, _test = data_notes[i][:int(n_notes * (1 - data_split_ratio['test']))], data_notes[i][int(n_notes * (1 - data_split_ratio['test'])):]\n",
        "    _train, _validation = _train[:int(len(_train) * data_split_ratio['train'])], _train[int(len(_train) * data_split_ratio['train']):]\n",
        "    n__train = len(_train)\n",
        "    n__validation = len(_validation)\n",
        "    n__test = len(_test)\n",
        "    data_dict = {'train' : (n__train, _train), 'validation' : (n__validation, _validation), 'test' : (n__test, _test)}\n",
        "    data_notes_ds[data_type[i]] = data_dict\n",
        "    \n",
        "    \n",
        "print()\n",
        "print('End!')\n",
        "\n",
        "print(f'total process time for {len(musician_filenames) + len(elec_filenames)} files : {time.time() - start_time:.3f}sec')\n",
        "# data_notes_ds Info\n",
        "# (n_data, tfds) 원소,\n",
        "# musician ==> train, validation, test\n",
        "# elec ==> train, validation, test\n",
        "# 총 6개의 원소가 들어가 있음"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMeKq5IZDheC",
        "outputId": "1883e085-b6f9-45ad-f717-651d6e490d82"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7126318"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "n_notes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "RMnql0GoixPU",
        "outputId": "0b918dbe-1b03-41e1-f4f9-abb271c7bd53"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pitch</th>\n",
              "      <th>step</th>\n",
              "      <th>duration</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>71</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.545933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>67</td>\n",
              "      <td>0.002137</td>\n",
              "      <td>2.543796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>59</td>\n",
              "      <td>0.002137</td>\n",
              "      <td>2.541659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>50</td>\n",
              "      <td>0.004273</td>\n",
              "      <td>2.537386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>43</td>\n",
              "      <td>0.006410</td>\n",
              "      <td>2.530975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3818</th>\n",
              "      <td>76</td>\n",
              "      <td>0.002671</td>\n",
              "      <td>1.071046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3819</th>\n",
              "      <td>73</td>\n",
              "      <td>0.004006</td>\n",
              "      <td>0.066773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3820</th>\n",
              "      <td>54</td>\n",
              "      <td>0.002671</td>\n",
              "      <td>0.064103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3821</th>\n",
              "      <td>62</td>\n",
              "      <td>0.009348</td>\n",
              "      <td>1.055020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3822</th>\n",
              "      <td>74</td>\n",
              "      <td>0.368589</td>\n",
              "      <td>0.686431</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4560843 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      pitch      step  duration\n",
              "0        71  0.000000  2.545933\n",
              "1        67  0.002137  2.543796\n",
              "2        59  0.002137  2.541659\n",
              "3        50  0.004273  2.537386\n",
              "4        43  0.006410  2.530975\n",
              "...     ...       ...       ...\n",
              "3818     76  0.002671  1.071046\n",
              "3819     73  0.004006  0.066773\n",
              "3820     54  0.002671  0.064103\n",
              "3821     62  0.009348  1.055020\n",
              "3822     74  0.368589  0.686431\n",
              "\n",
              "[4560843 rows x 3 columns]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_notes_ds['musician']['train'][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "q18an7qlE7Ne",
        "outputId": "0d0446ea-2d1b-4f4f-bb89-c2c345ae0aeb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pitch</th>\n",
              "      <th>step</th>\n",
              "      <th>duration</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>71</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.096875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>55</td>\n",
              "      <td>0.186458</td>\n",
              "      <td>0.217708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>71</td>\n",
              "      <td>0.009375</td>\n",
              "      <td>0.505208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>59</td>\n",
              "      <td>0.175000</td>\n",
              "      <td>0.167708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>62</td>\n",
              "      <td>0.169792</td>\n",
              "      <td>0.119792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8359</th>\n",
              "      <td>53</td>\n",
              "      <td>0.082031</td>\n",
              "      <td>0.157552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8360</th>\n",
              "      <td>51</td>\n",
              "      <td>0.160156</td>\n",
              "      <td>0.036458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8361</th>\n",
              "      <td>79</td>\n",
              "      <td>0.014323</td>\n",
              "      <td>0.115885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8362</th>\n",
              "      <td>80</td>\n",
              "      <td>0.134115</td>\n",
              "      <td>0.131510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8363</th>\n",
              "      <td>67</td>\n",
              "      <td>0.113281</td>\n",
              "      <td>0.026042</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4560843 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      pitch      step  duration\n",
              "0        71  0.000000  0.096875\n",
              "1        55  0.186458  0.217708\n",
              "2        71  0.009375  0.505208\n",
              "3        59  0.175000  0.167708\n",
              "4        62  0.169792  0.119792\n",
              "...     ...       ...       ...\n",
              "8359     53  0.082031  0.157552\n",
              "8360     51  0.160156  0.036458\n",
              "8361     79  0.014323  0.115885\n",
              "8362     80  0.134115  0.131510\n",
              "8363     67  0.113281  0.026042\n",
              "\n",
              "[4560843 rows x 3 columns]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_notes_ds['elec']['train'][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oo10ivd9FHKk",
        "outputId": "b3d09e9f-e4b0-4c37-ff56-cd3594ec7c20"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7126318"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(data_notes_ds['elec']['train'][1]) + len(data_notes_ds['elec']['test'][1]) + len(data_notes_ds['elec']['validation'][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPva8z5pFY9y",
        "outputId": "045c0366-8180-4dc8-e4b4-7c7527ca4b97"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "14341631"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(data_notes_ds['musician']['train'][1]) + len(data_notes_ds['musician']['test'][1]) + len(data_notes_ds['musician']['validation'][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RG1_6ydKW5Fn"
      },
      "outputs": [],
      "source": [
        "# SEQ_LENGTH = 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYFg_fHlkKY2"
      },
      "source": [
        "# cycleGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "EqTNbKpcogo-"
      },
      "outputs": [],
      "source": [
        "# v2\n",
        "def get_generator(model_name, output_channels=OUTPUT_CHANNELS):\n",
        "  input_shape = (SEQ_LENGTH, output_channels)\n",
        "  # print(input_shape)\n",
        "  inputs = tf.keras.Input(input_shape)\n",
        "\n",
        "  x0 = tf.keras.layers.LSTM(524, return_sequences=True)(inputs)\n",
        "  x1 = tf.keras.layers.LSTM(256, return_sequences=True)(x0)\n",
        "  x2 = tf.keras.layers.LSTM(128, return_sequences=True)(x1)\n",
        "  x3 = tf.keras.layers.LSTM(SEQ_LENGTH, return_sequences=True)(x2)\n",
        "  outputs = tf.keras.layers.Dense(3, name='all')(x3)\n",
        "\n",
        "  model = tf.keras.Model(inputs, outputs, name=model_name)\n",
        "  return model\n",
        "\n",
        "\n",
        "# 향후 수정 후, 사용하기\n",
        "def get_discriminator(model_name, output_channels=OUTPUT_CHANNELS):\n",
        "    input_shape = (SEQ_LENGTH, output_channels)\n",
        "    inputs = tf.keras.Input(input_shape)\n",
        "    x0 = tf.keras.layers.LSTM(524, return_sequences=True)(inputs)\n",
        "    x1 = tf.keras.layers.LSTM(256, return_sequences=True)(x0)\n",
        "    x2 = tf.keras.layers.LSTM(128, return_sequences=True)(x1)\n",
        "    outputs = tf.keras.layers.LSTM(SEQ_LENGTH, return_sequences=True)(x2)\n",
        "\n",
        "    model = tf.keras.Model(inputs, outputs, name=model_name)\n",
        "    return model\n",
        "    \n",
        "\n",
        "# 향후 종류를 다양하게 변형하기\n",
        "gen_musician = get_generator('gen_musician')\n",
        "gen_elec = get_generator('gen_elec')\n",
        "\n",
        "disc_musician = get_discriminator('disc_musician')\n",
        "disc_elec = get_discriminator('disc_elec')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mh_g0wKQTEup",
        "outputId": "5ad02057-567d-4e5b-bace-2ffb759dd813"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"gen_musician\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 512, 3)]          0         \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 512, 524)          1106688   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 512, 256)          799744    \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 512, 128)          197120    \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 512, 512)          1312768   \n",
            "                                                                 \n",
            " all (Dense)                 (None, 512, 3)            1539      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,417,859\n",
            "Trainable params: 3,417,859\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "gen_musician.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "18kz0PBwkKY3"
      },
      "outputs": [],
      "source": [
        "# loss function\n",
        "\n",
        "loss_obj = tf.keras.losses.BinaryCrossentropy()\n",
        "# losses_ratio = {'disc_loss' : 1.0, 'gen_loss' : 1.0, 'cycle_loss' : 1.0}\n",
        "\n",
        "def disc_loss(real, generated):\n",
        "\n",
        "    real_loss = loss_obj(tf.ones_like(real), real)\n",
        "    fake_loss = loss_obj(tf.zeros_like(generated), generated)\n",
        "    \n",
        "    total_loss = real_loss + fake_loss\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "def gen_loss(generated):\n",
        "    return loss_obj(tf.ones_like(generated), generated)\n",
        "\n",
        "def cycle_loss(real_data, cycled_data):\n",
        "    return tf.reduce_mean(tf.abs(real_data - cycled_data))\n",
        "\n",
        "def iden_loss(real_data, gen_from_real_data):\n",
        "    return tf.reduce_mean(tf.abs(real_data - gen_from_real_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "XZent3AukKY3"
      },
      "outputs": [],
      "source": [
        "# optimizer\n",
        "\n",
        "# LR = 2e-4\n",
        "# B1 = 0.5 \n",
        "\n",
        "gen_musician_opt = tf.keras.optimizers.Adam(learning_rate=LR, beta_1=B1)\n",
        "gen_elec_opt = tf.keras.optimizers.Adam(learning_rate=LR, beta_1=B1)\n",
        "\n",
        "disc_musician_opt = tf.keras.optimizers.Adam(learning_rate=LR, beta_1=B1)\n",
        "disc_elec_opt = tf.keras.optimizers.Adam(learning_rate=LR, beta_1=B1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "D7nFJI6xkKY3"
      },
      "outputs": [],
      "source": [
        "# checkpoint_path의 경우 계속해서 바꿔줘야 에러가 나지 않음, 이 cell(+위에 model 정의 cell)을 반복해서 실행하면 error 발생함\n",
        "checkpoint_path = \"./checkpoint/train_2022_11_10\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(gen_musician=gen_musician,\n",
        "                           gen_elec=gen_elec,\n",
        "                           disc_musician=disc_musician,\n",
        "                           disc_elec=disc_elec,\n",
        "                           gen_musician_opt=gen_musician_opt,\n",
        "                           gen_elec_opt=gen_elec_opt,\n",
        "                           disc_musician_opt=disc_musician_opt,\n",
        "                           disc_elec_opt=disc_elec_opt)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print ('Latest checkpoint restored!!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "bfagpkxhkKY3"
      },
      "outputs": [],
      "source": [
        "# real_x : musician data\n",
        "# real_y : elec data\n",
        "# g : gen elec\n",
        "# f : gen music\n",
        "\n",
        "@tf.function\n",
        "def train_step(real_x, real_y, losses_ratio):\n",
        "    # persistent is set to True because the tape is used more than\n",
        "    # once to calculate the gradients.\n",
        "    all_loss = []\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        # Generator G translates X -> Y\n",
        "        # Generator F translates Y -> X.\n",
        "        fake_y = gen_elec(real_x, training=True)\n",
        "        cycled_x = gen_musician(fake_y, training=True)\n",
        "\n",
        "        fake_x = gen_musician(real_y, training=True)\n",
        "        cycled_y = gen_elec(fake_x, training=True)\n",
        "\n",
        "        # same_x and same_y are used for identity loss.\n",
        "        same_x = gen_musician(real_x, training=True)\n",
        "        same_y = gen_elec(real_y, training=True)\n",
        "\n",
        "\n",
        "        disc_real_x = disc_musician(real_x, training=True)\n",
        "        disc_real_y = disc_elec(real_y, training=True)\n",
        "\n",
        "        disc_fake_x = disc_musician(fake_x, training=True)\n",
        "        disc_fake_y = disc_elec(fake_y, training=True)\n",
        "\n",
        "        # calculate the loss\n",
        "        gen_elec_loss = gen_loss(disc_fake_y)\n",
        "        gen_musician_loss = gen_loss(disc_fake_x)\n",
        "        total_cycle_loss = cycle_loss(real_x, cycled_x) + cycle_loss(real_y, cycled_y)\n",
        "\n",
        "        elec_iden_loss = iden_loss(real_y, same_y)\n",
        "        musician_iden_loss = iden_loss(real_x, same_x)\n",
        "\n",
        "        # Total generator loss = adversarial loss + cycle loss\n",
        "        total_gen_elec_loss = gen_elec_loss*losses_ratio['gen'] + total_cycle_loss*losses_ratio['cyc'] + elec_iden_loss*losses_ratio['iden']\n",
        "        total_gen_musician_loss = gen_musician_loss*losses_ratio['gen'] + total_cycle_loss*losses_ratio['cyc'] + musician_iden_loss*losses_ratio['iden']\n",
        "\n",
        "        disc_musician_loss = disc_loss(disc_real_x, disc_fake_x)*losses_ratio['disc']\n",
        "        disc_elec_loss = disc_loss(disc_real_y, disc_fake_y)*losses_ratio['disc']\n",
        "\n",
        "        all_loss = [tf.identity(gen_elec_loss), \n",
        "                    tf.identity(gen_musician_loss), \n",
        "                    tf.identity(total_cycle_loss), \n",
        "                    tf.identity(total_gen_elec_loss),\n",
        "                    tf.identity(total_gen_musician_loss),\n",
        "                    tf.identity(disc_musician_loss),\n",
        "                    tf.identity(disc_elec_loss),\n",
        "                    tf.identity(musician_iden_loss),\n",
        "                    tf.identity(elec_iden_loss)]\n",
        "\n",
        "\n",
        "    # Calculate the gradients for generator and discriminator\n",
        "    gen_elec_gradients = tape.gradient(total_gen_elec_loss, \n",
        "                                          gen_elec.trainable_variables)\n",
        "    \n",
        "    gen_musician_gradients = tape.gradient(total_gen_musician_loss, \n",
        "                                          gen_musician.trainable_variables)\n",
        "    \n",
        "    disc_musician_gradients = tape.gradient(disc_musician_loss, \n",
        "                                              disc_musician.trainable_variables)\n",
        "    disc_elec_gradients = tape.gradient(disc_elec_loss, \n",
        "                                              disc_elec.trainable_variables)\n",
        "\n",
        "    # Apply the gradients to the optimizer\n",
        "    gen_elec_opt.apply_gradients(zip(gen_elec_gradients, \n",
        "                                              gen_elec.trainable_variables))\n",
        "\n",
        "    gen_musician_opt.apply_gradients(zip(gen_musician_gradients, \n",
        "                                              gen_musician.trainable_variables))\n",
        "    \n",
        "    disc_musician_opt.apply_gradients(zip(disc_musician_gradients,\n",
        "                                                  disc_musician.trainable_variables))\n",
        "    \n",
        "    disc_elec_opt.apply_gradients(zip(disc_elec_gradients,\n",
        "                                                  disc_elec.trainable_variables))\n",
        "    \n",
        "    return all_loss\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPFvnti5R31u",
        "outputId": "db177dca-3f48-46ba-c171-b96142549722"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "seq_train_notes shape (8907, 512, 3)\n",
            "\n",
            "notes_ds element spec TensorSpec(shape=(512, 3), dtype=tf.float64, name=None)\n",
            "\n",
            "train_ds spec TensorSpec(shape=(None, 512, 3), dtype=tf.float32, name=None)\n",
            "\n",
            "--------------------------------------------------\n",
            "seq_train_notes shape (8907, 512, 3)\n",
            "\n",
            "notes_ds element spec TensorSpec(shape=(512, 3), dtype=tf.float64, name=None)\n",
            "\n",
            "train_ds spec TensorSpec(shape=(None, 512, 3), dtype=tf.float32, name=None)\n",
            "\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def notes_to_trainable_dataset(all_notes, n_data):\n",
        "    key_order = ['pitch', 'step', 'duration']\n",
        "    train_notes = np.stack([all_notes[key] for key in key_order], axis=1)\n",
        "    n_notes, n_features = train_notes.shape \n",
        "\n",
        "    train_notes = train_notes[:n_notes - (n_notes % SEQ_LENGTH)]\n",
        "    train_notes[:, 0] = train_notes[:, 0] / VOCAB_SIZE\n",
        "\n",
        "    seq_train_notes = train_notes.reshape(-1, SEQ_LENGTH, 3)\n",
        "    # seq_train_notes = seq_train_notes / np.array([VOCAB_SIZE, 0.0, 0.0])\n",
        "\n",
        "    print(f'seq_train_notes shape {seq_train_notes.shape}')\n",
        "    print()\n",
        "\n",
        "    seq_train_notes = seq_train_notes[:n_data]\n",
        "    \n",
        "    seq_ds = tf.data.Dataset.from_tensor_slices(seq_train_notes)\n",
        "    print(f'notes_ds element spec {seq_ds.element_spec}')\n",
        "    print()\n",
        "\n",
        "    # BUFFER_SIZE = n_notes - SEQ_LENGTH\n",
        "    train_ds = seq_ds.map(lambda x: (tf.cast(x, tf.float32)), num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(BATCH_SIZE, num_parallel_calls=tf.data.experimental.AUTOTUNE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    print(f'train_ds spec {train_ds.element_spec}')\n",
        "    print()\n",
        "\n",
        "    print('-' * PRINT_ITER)\n",
        "\n",
        "    return train_ds\n",
        "\n",
        "\n",
        "musician_train_ds = notes_to_trainable_dataset(data_notes_ds['musician']['train'][1], 100)\n",
        "\n",
        "elec_train_ds = notes_to_trainable_dataset(data_notes_ds['elec']['train'][1], 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "8hTlN9117HLi"
      },
      "outputs": [],
      "source": [
        "# @tf.function을 이용하여 code를 실행하는 도중에 .nmupy()와 같은 code 실행 가능\n",
        "# https://github.com/tgjeon/TF-Eager-Execution-Guide-KR/blob/master/guide.md 참고\n",
        "# https://www.tensorflow.org/api_docs/python/tf/config/run_functions_eagerly\n",
        "tf.config.run_functions_eagerly(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "VhRzDcg52pmu"
      },
      "outputs": [],
      "source": [
        "from tensorboardX import SummaryWriter\n",
        "writer = SummaryWriter('runs_test/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "k4RFr6e08mME"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXWXiV6NW5Fr",
        "outputId": "f640aeed-cb4c-4d88-e68a-3c5f63671c26"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "556.734375"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "2 * (35631 / 128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgGnFi2fW5Fr",
        "outputId": "c13c846d-c631-4a2f-b368-8be4c0230150"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5938.5"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "2 * (35631 / BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lc1xCXbDkKY3",
        "outputId": "72c91abc-fa69-48a6-82e8-eb8b34ed4fab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ERROR: Failed to launch TensorBoard (exited with 1).\n",
              "Contents of stderr:\n",
              "2022-11-21 08:18:52.827729: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
              "2022-11-21 08:18:52.834120: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
              "2022-11-21 08:18:58.660702: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found\n",
              "2022-11-21 08:18:58.663898: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
              "2022-11-21 08:18:58.683424: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: DESKTOP-JSMT29H\n",
              "2022-11-21 08:18:58.685637: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: DESKTOP-JSMT29H\n",
              "Traceback (most recent call last):\n",
              "  File \"C:\\users\\gobak\\appdata\\local\\programs\\python\\python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
              "    return _run_code(code, main_globals, None,\n",
              "  File \"C:\\users\\gobak\\appdata\\local\\programs\\python\\python39\\lib\\runpy.py\", line 87, in _run_code\n",
              "    exec(code, run_globals)\n",
              "  File \"c:\\Users\\gobak\\.virtualenvs\\.venv-HOEqjjdC\\Scripts\\tensorboard.exe\\__main__.py\", line 7, in <module>\n",
              "  File \"C:\\Users\\gobak\\.virtualenvs\\.venv-HOEqjjdC\\lib\\site-packages\\tensorboard\\main.py\", line 46, in run_main\n",
              "    app.run(tensorboard.main, flags_parser=tensorboard.configure)\n",
              "  File \"C:\\Users\\gobak\\.virtualenvs\\.venv-HOEqjjdC\\lib\\site-packages\\absl\\app.py\", line 308, in run\n",
              "    _run_main(main, args)\n",
              "  File \"C:\\Users\\gobak\\.virtualenvs\\.venv-HOEqjjdC\\lib\\site-packages\\absl\\app.py\", line 254, in _run_main\n",
              "    sys.exit(main(argv))\n",
              "  File \"C:\\Users\\gobak\\.virtualenvs\\.venv-HOEqjjdC\\lib\\site-packages\\tensorboard\\program.py\", line 276, in main\n",
              "    return runner(self.flags) or 0\n",
              "  File \"C:\\Users\\gobak\\.virtualenvs\\.venv-HOEqjjdC\\lib\\site-packages\\tensorboard\\program.py\", line 292, in _run_serve_subcommand\n",
              "    server = self._make_server()\n",
              "  File \"C:\\Users\\gobak\\.virtualenvs\\.venv-HOEqjjdC\\lib\\site-packages\\tensorboard\\program.py\", line 467, in _make_server\n",
              "    app = application.TensorBoardWSGIApp(\n",
              "  File \"C:\\Users\\gobak\\.virtualenvs\\.venv-HOEqjjdC\\lib\\site-packages\\tensorboard\\backend\\application.py\", line 140, in TensorBoardWSGIApp\n",
              "    return TensorBoardWSGI(\n",
              "  File \"C:\\Users\\gobak\\.virtualenvs\\.venv-HOEqjjdC\\lib\\site-packages\\tensorboard\\backend\\application.py\", line 253, in __init__\n",
              "    raise ValueError(\n",
              "ValueError: Duplicate plugins for name projector"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [25], line 36\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39mfor\u001b[39;00m musician, elec \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mzip((musician_train_ds, elec_train_ds)):\n\u001b[0;32m     31\u001b[0m     \u001b[39m# musician = tf.cast(musician, dtype=tf.float32)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     \u001b[39m# elec = tf.cast(elec, dtype=tf.float32)\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     all_loss \u001b[39m=\u001b[39m [\u001b[39m0\u001b[39m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(all_loss_name))]   \n\u001b[1;32m---> 36\u001b[0m     for_all_loss \u001b[39m=\u001b[39m train_step(musician, elec, losses_ratio)\n\u001b[0;32m     37\u001b[0m     \u001b[39mfor\u001b[39;00m i, loss \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(for_all_loss):\n\u001b[0;32m     38\u001b[0m         all_loss[i] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mitem()\n",
            "File \u001b[1;32mc:\\Users\\gobak\\.virtualenvs\\.venv-HOEqjjdC\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\gobak\\.virtualenvs\\.venv-HOEqjjdC\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:892\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    890\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_functions_eagerly:\n\u001b[0;32m    891\u001b[0m   \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name, tf_function_call\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39meager\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 892\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_python_function(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    894\u001b[0m \u001b[39m# Only count the statistics the first time, before initialization took\u001b[39;00m\n\u001b[0;32m    895\u001b[0m \u001b[39m# place.\u001b[39;00m\n\u001b[0;32m    896\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "Cell \u001b[1;32mIn [20], line 58\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(real_x, real_y, losses_ratio)\u001b[0m\n\u001b[0;32m     46\u001b[0m     all_loss \u001b[39m=\u001b[39m [tf\u001b[39m.\u001b[39midentity(gen_elec_loss), \n\u001b[0;32m     47\u001b[0m                 tf\u001b[39m.\u001b[39midentity(gen_musician_loss), \n\u001b[0;32m     48\u001b[0m                 tf\u001b[39m.\u001b[39midentity(total_cycle_loss), \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     53\u001b[0m                 tf\u001b[39m.\u001b[39midentity(musician_iden_loss),\n\u001b[0;32m     54\u001b[0m                 tf\u001b[39m.\u001b[39midentity(elec_iden_loss)]\n\u001b[0;32m     57\u001b[0m \u001b[39m# Calculate the gradients for generator and discriminator\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m gen_elec_gradients \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39;49mgradient(total_gen_elec_loss, \n\u001b[0;32m     59\u001b[0m                                       gen_elec\u001b[39m.\u001b[39;49mtrainable_variables)\n\u001b[0;32m     61\u001b[0m gen_musician_gradients \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39mgradient(total_gen_musician_loss, \n\u001b[0;32m     62\u001b[0m                                       gen_musician\u001b[39m.\u001b[39mtrainable_variables)\n\u001b[0;32m     64\u001b[0m disc_musician_gradients \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39mgradient(disc_musician_loss, \n\u001b[0;32m     65\u001b[0m                                           disc_musician\u001b[39m.\u001b[39mtrainable_variables)\n",
            "File \u001b[1;32mc:\\Users\\gobak\\.virtualenvs\\.venv-HOEqjjdC\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:1113\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1107\u001b[0m   output_gradients \u001b[39m=\u001b[39m (\n\u001b[0;32m   1108\u001b[0m       composite_tensor_gradient\u001b[39m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[0;32m   1109\u001b[0m           output_gradients))\n\u001b[0;32m   1110\u001b[0m   output_gradients \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m ops\u001b[39m.\u001b[39mconvert_to_tensor(x)\n\u001b[0;32m   1111\u001b[0m                       \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m output_gradients]\n\u001b[1;32m-> 1113\u001b[0m flat_grad \u001b[39m=\u001b[39m imperative_grad\u001b[39m.\u001b[39;49mimperative_grad(\n\u001b[0;32m   1114\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tape,\n\u001b[0;32m   1115\u001b[0m     flat_targets,\n\u001b[0;32m   1116\u001b[0m     flat_sources,\n\u001b[0;32m   1117\u001b[0m     output_gradients\u001b[39m=\u001b[39;49moutput_gradients,\n\u001b[0;32m   1118\u001b[0m     sources_raw\u001b[39m=\u001b[39;49mflat_sources_raw,\n\u001b[0;32m   1119\u001b[0m     unconnected_gradients\u001b[39m=\u001b[39;49munconnected_gradients)\n\u001b[0;32m   1121\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persistent:\n\u001b[0;32m   1122\u001b[0m   \u001b[39m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[0;32m   1123\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_watched_variables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tape\u001b[39m.\u001b[39mwatched_variables()\n",
            "File \u001b[1;32mc:\\Users\\gobak\\.virtualenvs\\.venv-HOEqjjdC\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     65\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mUnknown value for unconnected_gradients: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m unconnected_gradients)\n\u001b[1;32m---> 67\u001b[0m \u001b[39mreturn\u001b[39;00m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_TapeGradient(\n\u001b[0;32m     68\u001b[0m     tape\u001b[39m.\u001b[39;49m_tape,  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m     69\u001b[0m     target,\n\u001b[0;32m     70\u001b[0m     sources,\n\u001b[0;32m     71\u001b[0m     output_gradients,\n\u001b[0;32m     72\u001b[0m     sources_raw,\n\u001b[0;32m     73\u001b[0m     compat\u001b[39m.\u001b[39;49mas_str(unconnected_gradients\u001b[39m.\u001b[39;49mvalue))\n",
            "File \u001b[1;32mc:\\Users\\gobak\\.virtualenvs\\.venv-HOEqjjdC\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:160\u001b[0m, in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    158\u001b[0m     gradient_name_scope \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m forward_pass_name_scope \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    159\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(gradient_name_scope):\n\u001b[1;32m--> 160\u001b[0m     \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39;49mout_grads)\n\u001b[0;32m    161\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m   \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39mout_grads)\n",
            "File \u001b[1;32mc:\\Users\\gobak\\.virtualenvs\\.venv-HOEqjjdC\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1736\u001b[0m, in \u001b[0;36m_MatMulGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m   1734\u001b[0m b \u001b[39m=\u001b[39m math_ops\u001b[39m.\u001b[39mconj(op\u001b[39m.\u001b[39minputs[\u001b[39m1\u001b[39m])\n\u001b[0;32m   1735\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m t_a \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m t_b:\n\u001b[1;32m-> 1736\u001b[0m   grad_a \u001b[39m=\u001b[39m gen_math_ops\u001b[39m.\u001b[39;49mmat_mul(grad, b, transpose_b\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m   1737\u001b[0m   grad_b \u001b[39m=\u001b[39m gen_math_ops\u001b[39m.\u001b[39mmat_mul(a, grad, transpose_a\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   1738\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m t_a \u001b[39mand\u001b[39;00m t_b:\n",
            "File \u001b[1;32mc:\\Users\\gobak\\.virtualenvs\\.venv-HOEqjjdC\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:6012\u001b[0m, in \u001b[0;36mmat_mul\u001b[1;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[0;32m   6010\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m   6011\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 6012\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m   6013\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mMatMul\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, a, b, \u001b[39m\"\u001b[39;49m\u001b[39mtranspose_a\u001b[39;49m\u001b[39m\"\u001b[39;49m, transpose_a, \u001b[39m\"\u001b[39;49m\u001b[39mtranspose_b\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   6014\u001b[0m       transpose_b)\n\u001b[0;32m   6015\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   6016\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# tensorboard graph color rgb : #1fddff\n",
        "\n",
        "losses_ratio = {'disc' : 1.0, 'gen' : 1.0, 'cyc' : 1.0, 'iden' : 1.0} # cycle, iden loss ratio는 서로 종속적임(in tf document)\n",
        "all_loss = []\n",
        "all_loss_name = ['gen_elec_loss', \n",
        "                 'gen_musician_loss',\n",
        "                 'total_cycle_loss', \n",
        "                 'total_gen_elec_loss',\n",
        "                 'total_gen_musician_loss',\n",
        "                 'disc_musician_loss',\n",
        "                 'disc_elec_loss',\n",
        "                 'musician_iden_loss',\n",
        "                 'elec_iden_loss']\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs_test --port=6006\n",
        "\n",
        "# 단순히 모델의 가중치 업데이트 횟수를 의미함\n",
        "# 향후 이 값을 바탕으로 epoch를 계산 가능 | \n",
        "# update_n = epoch * (n_notes / (seq_length * batch_size)) = epoch * (n_seq_notes / batch_size)\n",
        "\n",
        "update_cnt = 0\n",
        "\n",
        "total_start_time = time.time()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    n = 0\n",
        "    \n",
        "    for musician, elec in tf.data.Dataset.zip((musician_train_ds, elec_train_ds)):\n",
        "        # musician = tf.cast(musician, dtype=tf.float32)\n",
        "        # elec = tf.cast(elec, dtype=tf.float32)\n",
        "                \n",
        "        all_loss = [0 for _ in range(len(all_loss_name))]   \n",
        "\n",
        "        for_all_loss = train_step(musician, elec, losses_ratio)\n",
        "        for i, loss in enumerate(for_all_loss):\n",
        "            all_loss[i] += loss.numpy().item()\n",
        "\n",
        "        if update_cnt % 5 == 0:\n",
        "            print('-', end='')\n",
        "        \n",
        "        n += 1\n",
        "        update_cnt += 1\n",
        "\n",
        "        # 아래 부분에서 BATCH_SIZE로 나눌 지에 관해 고민 필요\n",
        "        all_loss = [loss for loss in all_loss]\n",
        "        for loss, loss_name in zip(all_loss, all_loss_name):\n",
        "            writer.add_scalar(loss_name, loss, update_cnt)\n",
        "\n",
        "    # clear_output(wait=True)\n",
        "    \n",
        "    # note를 시각화할 수 있는 거 넣기\n",
        "    # ex) generate_images(generator_g, sample_horse)\n",
        "    \n",
        "    if (epoch + 1) % 3 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print(f'Saving checkpoint for each {epoch + 1} at {ckpt_save_path}')\n",
        "    \n",
        "    print(f'Time taken for epoch {epoch + 1} is {time.time() - start:.3f} sec\\n')\n",
        "\n",
        "print()\n",
        "print('-' * PRINT_ITER)\n",
        "print(f'Time taken for total {EPOCHS} epoch : {time.time() - total_start_time:.3f} sec')\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Za3s9whV6pH"
      },
      "outputs": [],
      "source": [
        "def generate_domain_notes(input_ds, model):\n",
        "    result_list = []\n",
        "    generated_notes_list = []\n",
        "    n_batch_data = tf.data.experimental.cardinality(input_ds).numpy().item()\n",
        "    # n_batch_data = 1\n",
        "\n",
        "    for inp in input_ds.take(n_batch_data):\n",
        "        gen_music = model(inp)\n",
        "        result_list.append(gen_music)\n",
        "\n",
        "    for result in result_list:\n",
        "        pitchs = result[:, :, 0]\n",
        "        steps = result[:, :, 1]\n",
        "        durations = result[:, :, 2]\n",
        "\n",
        "        pitchs = tf.reshape(pitchs, [-1])\n",
        "        pitchs = tf.math.scalar_mul(VOCAB_SIZE, pitchs)\n",
        "        pitchs = tf.math.round(pitchs)\n",
        "        pitchs = tf.cast(pitchs, dtype=tf.int32)\n",
        "        pitchs = pitchs.numpy()\n",
        "\n",
        "        durations = tf.reshape(durations, [-1])\n",
        "        durations = durations.numpy()\n",
        "\n",
        "        steps = tf.reshape(steps, [-1])\n",
        "        steps = steps.numpy() \n",
        "\n",
        "        generated_notes = []\n",
        "        prev_start = 0\n",
        "\n",
        "        for pitch, duration, step in zip(pitchs, durations, steps):\n",
        "            pitch, duration, step = pitch.item(), duration.item(), step.item()\n",
        "\n",
        "            if pitch > VOCAB_SIZE - 1:\n",
        "                pitch = VOCAB_SIZE - 1\n",
        "            if pitch < 0:\n",
        "                pitch = 0\n",
        "                \n",
        "            start = prev_start + step \n",
        "            end = start + duration\n",
        "            input_note = (pitch, step, duration)\n",
        "            generated_notes.append((*input_note, start, end))\n",
        "\n",
        "            prev_start = start\n",
        "\n",
        "        generated_notes = pd.DataFrame(\n",
        "            generated_notes, columns=(*key_order, 'start', 'end')\n",
        "        )\n",
        "\n",
        "        generated_notes_list.append(generated_notes)\n",
        "\n",
        "    generated_notes_all = pd.concat(generated_notes_list, ignore_index=True)\n",
        "    return generated_notes_all\n",
        "\n",
        "def notes_to_testable_dataset(all_notes):\n",
        "    key_order = ['pitch', 'step', 'duration']\n",
        "    test_notes = np.stack([all_notes[key] for key in key_order], axis=1)\n",
        "    n_notes, n_features = test_notes.shape \n",
        "    \n",
        "    n_notes_for_seq = n_notes - (n_notes % SEQ_LENGTH)\n",
        "    test_notes = test_notes[:n_notes_for_seq]\n",
        "    \n",
        "    test_notes = test_notes.reshape(-1, SEQ_LENGTH, 3)\n",
        "    print(f'test_notes shape {test_notes.shape}')\n",
        "    print()\n",
        "    \n",
        "    notes_ds = tf.data.Dataset.from_tensor_slices(test_notes)\n",
        "    print(f'notes_ds element spec {notes_ds.element_spec}')\n",
        "    print()\n",
        "\n",
        "    seq_ds = notes_ds\n",
        "    print(f'seq_ds element_spec {seq_ds.element_spec}')\n",
        "    print()\n",
        "\n",
        "    # BUFFER_SIZE = n_notes - SEQ_LENGTH\n",
        "    test_ds = seq_ds.batch(n_notes_for_seq, num_parallel_calls=tf.data.experimental.AUTOTUNE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    print(f'test_ds spec {test_ds.element_spec}')\n",
        "    print()\n",
        "\n",
        "    print('-' * PRINT_ITER)\n",
        "\n",
        "    return test_ds\n",
        "\n",
        "def generate_notes(midi_filepath, model):\n",
        "    notes = midi_to_notes(midi_filepath)\n",
        "    test_ds = notes_to_testable_dataset(notes)\n",
        "    generated_notes = generate_domain_notes(test_ds, model)\n",
        "    return generated_notes\n",
        "    \n",
        "\n",
        "midi_filepath = elec_filenames[0]\n",
        "\n",
        "generated_notes = generate_notes(midi_filepath, gen_musician)\n",
        "generated_notes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRnwJQdSlKFX",
        "outputId": "31e459c1-3505-4465-e321-fdeeb5f8b240"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun Nov 20 10:37:02 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   70C    P0    31W /  70W |   2810MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6a8Z-6MOTBCb"
      },
      "outputs": [],
      "source": [
        "out_file = 'output.mid'\n",
        "out_pm = notes_to_midi(\n",
        "    generated_notes, out_file=out_file, instrument_name=instrument_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VE5Zd7tzTHun"
      },
      "outputs": [],
      "source": [
        "# # generator 불러오기\n",
        "# gen_musician_ckpt_path = ''\n",
        "# gen_musician.load_weights(gen_musician_ckpt_path)\n",
        "\n",
        "# gen_elec_ckpt_path = '' \n",
        "# gen_elec.load_weights(gen_elec_ckpt_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sctRLZFeDheR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuaIw8UXDheS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "76d2a536684e05cb77c92a45f34021a8fe6e78c156cd0f72220424a41f6bdcdc"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
