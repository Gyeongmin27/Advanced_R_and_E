{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPnX8kayLjGC"
   },
   "source": [
    "# Version Info\n",
    "v5 version에서 cycleGAN 부분을 아래 notebook을 바탕으로 수정 중 == v6 version<br>https://colab.research.google.com/drive/1n5OzIaic1Ho60UAtqEgCnRyn7Lg1Po_v<br>sequence 길이를 모두 1로 맞춰야 함, 그렇지 않으면 생성할 때 마다 차원이 감소함. 아니면 train 과정에서 모델이 seq 길이 만큼 하나의 데이터에 대해 반복을 수행해서 생성을 진행해야 한다.\n",
    "\n",
    "v6 version lstm cell return_sequnces=True로 설정하고 마지막 output layer에서 2차원 dense를 수행<br> --> seq_l을 데이터의 차원으로 갖도록 함\n",
    "\n",
    "cycleGAN의 generation 부분 -> v7\n",
    "\n",
    "cycleGAN의 loss 및 훈련 -> v8\n",
    "\n",
    "gradients tape 문제 해결을 위한 tf dataset 이용 -> v9\n",
    "\n",
    "v9의 cycleGAN pt type code 정리 및 파이프라인 보수 -> v10<br>\n",
    "v10에서 seq화 과정에서 발생하는 데이터 손실, loss 그래프, gpu 사용량 최적화, notes to MIDI(생성 데이터 MIDI로 변환) 수정 -> v11<br>\n",
    "v11에서 todo 6~ 까지의 수정 -> v12<br>\n",
    "v12에서 vocab size 관련 error 수정 + batch 등의 파라미터 조정 -> v13<br>\n",
    "v13에서 만든 pt type 완성본을 여러가지 방면으로 실험 -> v14<br>\n",
    "v14에서 실험+ -> v15<br>\n",
    "v15에서 seq, batch, bi, gru, tf.reduce_mean 실험 pt type(그나마 괜찮은 값) -> v16<br>\n",
    "v16에서 심알 포스터에 들어갈 자료 생성 -> v17<br>\n",
    "v17에서 발생하는 생성자의 학습 문제 및 과적합 문제 해결을 위해 판별자 lr 낮추기 -> 판별자의 학습을 늦춰 생성자의 학습 시간을 벌어주기, \n",
    "binary crossentropy 쪽에서 발생 또는 다른 loss 계산에서 발생하는 nan or inf 문제를 해결하기 위해 loss function 수정 -> v18<br>\n",
    "v18 + TimeDistributed model 적용한 거 -> v19<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoiMShYhCSSw"
   },
   "source": [
    "# TODO\n",
    "1. 일반 LSTM model과는 다르게 cycleGAN의 input data를 만들어야 한다.\n",
    "즉, sequence data로 data를 변환할 때, 일반 LSTM model과는 달리 target data를 생성하지 않아야 하며 seq_L 만큼 마다 나눠서 데이터 손실 없이 input data로 사용해야한다.\n",
    "\n",
    "    -> 모델이 seq_l 길이 만큼의 데이터를 바탕으로 (그 길이 안 데이터 해석) output을 만들어 낸다.<br> 이후 sequence data로 나뉘어진 output data를 다시 reshape한 후, midi로 변환하면 된다.\n",
    "\n",
    "2. sequence data 및 음악적 특성을 충분히 고려할 수 있도록 discriminator 및 그에 따른 loss function 구현 필요 -- 시간 계획 필수 (기한 고려)\n",
    "\n",
    "3. 향후 cycleGAN과 관련된 loss(cycle consistnecy 등)도 손을 볼 수 있으면 좋을 것 같다.\n",
    "\n",
    "4. GPU 사용량 늘리기\n",
    "\n",
    "5. batch를 만들때, shuffle 주의하기, test data에는 사용하지 않아야 할수도 있다. -> train data shuffle을 수행해도 무방할 정도의 seq_l을 잡거나 shuffle을 수행하지 않아야 한다.\n",
    "\n",
    "6. reshape 이용해서 create sequence 부분 고치기\n",
    "\n",
    "7. 실제 연구 데이터를 사용할 때, 두 도메인의 데이터 길이 조심하기 - 학습과정에서\n",
    "\n",
    "8. notes_to_midi 잘 이용하기\n",
    "\n",
    "9. MIDI file 제대로, 전부 append 되는 지 확인하기 --> file_path glob 부분에서 slicing에 의한 문제 --> 해결\n",
    "\n",
    "10. cycleGAN 생성 과정 중에 pitch, step, duration의 loss 차이가 튼 영향을 미친다면 music_generation 노트북을 활용하여 loss 재정의하기\n",
    "\n",
    "11. 각 노래 midi 별로 나눠서 처리하거나, batch size 크기를 늘려 모델이 batch로 학습을 진행할 때, 각각의 노래가 섞이는 경우를 조금은 줄이는 것이 좋은지 실험 필요\n",
    "\n",
    "12. colab 환경에서 여러개의 notebook을 띄워 놓은 후, 각각의 변수들을 변경하면서 실험 진행<br>\n",
    "\n",
    "13. 12를 진행하는 과정에서 실험 리스트, 설계 과정, 결과 등을 모두 기록해야함\n",
    "\n",
    "14. colab notebook 최대 가용량 -> 실험 가능량 설계 -> 시간 계획\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLJm-R6HbtFC"
   },
   "source": [
    "# Experiment plan\n",
    "1. batch\n",
    "2. data<br>\n",
    "    1. count\n",
    "    2. type<br>\n",
    "        1. musician<br>\n",
    "            1. piano-e-competition\n",
    "        2. elec<br>\n",
    "            1. maestro\n",
    "3. generator model<br>\n",
    "    1. model structure<br>\n",
    "        1. type<br>\n",
    "            1. bidirectional\n",
    "            2. Dense\n",
    "            3. GRU\n",
    "        2. layer<br>\n",
    "            1. unit count\n",
    "    2. hyper parameter\n",
    "4. discriminator model\n",
    "    1. model structure<br>\n",
    "        1. type<br>\n",
    "            1. bidirectional\n",
    "            2. Dense\n",
    "            3. GRU\n",
    "        2. layer<br>\n",
    "            1. unit count\n",
    "    2. hyper parameter<br>\n",
    "    3. for music<br>\n",
    "        1. patchGAN\n",
    "5. loss function weight<br>\n",
    "    1. brute force method\n",
    "6. optimizer<br>\n",
    "    1. type\n",
    "    2. learning rate\n",
    "7. future plan<br>\n",
    "    1. 특정곡의 곡 또는 작곡관련 nlp 정보 -> embbding -> 새로운 층으로 concatenate -> multimodal<br>\n",
    "    2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aDxtxjFDDhdz",
    "outputId": "160d9720-9f5d-44ae-c664-a2871084250a"
   },
   "outputs": [],
   "source": [
    "# # for installing\n",
    "\n",
    "# !pip install -U -q pip\n",
    "# !pip install -q tensorboardX\n",
    "# !pip install -q git+https://github.com/tensorflow/examples.git\n",
    "# !pip install -q pretty_midi\n",
    "# !pip install -q knockknock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Vl3Sf8IYi2e",
    "outputId": "09fb77cc-3a56-4c9d-e889-1945337b96fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'piano-e-competition-midi-files' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/chelate1118/piano-e-competition-midi-files.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZkkfwClODhd4",
    "outputId": "c78378b8-0089-4a3d-bac7-bad77bc84eda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/src/gs21004/.virtualenvs/venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "zXcyh1z9Dhd4"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Qle2WAaUkKYp"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-05 09:38:00.821424: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-05 09:38:00.954161: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-12-05 09:38:00.990951: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-12-05 09:38:01.700195: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.2/lib64:/usr/local/cuda-11.2/lib64\n",
      "2022-12-05 09:38:01.700272: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.2/lib64:/usr/local/cuda-11.2/lib64\n",
      "2022-12-05 09:38:01.700279: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# import 과정 error -> AVX2를 이용한 tf compile 진행 시, 더 최적화 가능함을 알려줌 | 아래 링크 참조\n",
    "# http://erpcomputing.com/build-tensorflow-from-source--mkl-enabled\n",
    "\n",
    "#library import\n",
    "\n",
    "# for data and preprocessing\n",
    "import tensorflow_datasets as tfds\n",
    "import collections\n",
    "import glob\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import pretty_midi\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import MIDI_Pedal # 사용법 : MIDI_Pedal.apply('test.mid', 'output.mid')\n",
    "\n",
    "\n",
    "# for model\n",
    "# 설치가 git인 경우 local에서 설치한 후 .venv/Lib에 넣으면 됨, local pip 설치경로는 위에 있음\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, GRU, LeakyReLU, Activation, TimeDistributed\n",
    "\n",
    "# for the others\n",
    "# import fluidsynth # MIDI는 다른 거로 재생하기\n",
    "import datetime\n",
    "from IPython import display\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Optional, Sequence, Tuple\n",
    "import time\n",
    "import os\n",
    "# from knockknock import desktop_sender\n",
    "# from google.colab import files # google colab에서 결과 midi file 다운받고 싶을 때 import 하면 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BlXj0TQDlBvY",
    "outputId": "54d12bf0-bacd-4ddd-ce41-1098a536787b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 1282\n",
      "data/maestro-v2.0.0/2013/ORIG-MIDI_01_7_8_13_Group__MID--AUDIO_07_R2_2013_wav--2.midi\n"
     ]
    }
   ],
   "source": [
    "# 단순 model test용 데이터\n",
    "\n",
    "data_dir = pathlib.Path('data/maestro-v2.0.0')\n",
    "if not data_dir.exists():\n",
    "  tf.keras.utils.get_file(\n",
    "      'maestro-v2.0.0-midi.zip',\n",
    "      origin='https://storage.googleapis.com/magentadata/datasets/maestro/v2.0.0/maestro-v2.0.0-midi.zip',\n",
    "      extract=True,\n",
    "      cache_dir='.', cache_subdir='data',\n",
    "  )\n",
    "\n",
    "filenames = glob.glob(str(data_dir/'**/*.mid*'))\n",
    "print('Number of files:', len(filenames))\n",
    "\n",
    "sample_file = filenames[0]\n",
    "print(sample_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "asG7Fu5AEcsN",
    "outputId": "a9cf9c32-1d47-44ac-fd01-5b6c210e3667"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instruments: 1\n",
      "Instrument name: Acoustic Grand Piano\n"
     ]
    }
   ],
   "source": [
    "pm = pretty_midi.PrettyMIDI(sample_file)\n",
    "\n",
    "print('Number of instruments:', len(pm.instruments))\n",
    "instrument = pm.instruments[0]\n",
    "instrument_name = pretty_midi.program_to_instrument_name(instrument.program)\n",
    "print('Instrument name:', instrument_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "JBrfs9B2kKYq"
   },
   "outputs": [],
   "source": [
    "# global variable declaration\n",
    "\n",
    "#global\n",
    "seed = 42\n",
    "\n",
    "# for model\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "tf.random.set_seed(seed)\n",
    "BUFFER_SIZE = -1\n",
    "BATCH_SIZE = 1 # cycleGAN tf document 참고한 값 \n",
    "SEQ_LENGTH = 1024\n",
    "VOCAB_SIZE = 128\n",
    "\n",
    "N_FEATURE = OUTPUT_CHANNELS = 3\n",
    "LR_gen = 2e-4 # opt\n",
    "LR_disc = 2e-4\n",
    "# B1 = 0.05 # opt\n",
    "\n",
    "# for data\n",
    "np.random.seed(seed)\n",
    "# MUSICIAN_MIDI_FILES_DIR = pathlib.Path('data/maestro-v2.0.0')\n",
    "MUSICIAN_MIDI_FILES_DIR = pathlib.Path('piano_e_competition_midi_files/midi')\n",
    "ELEC_MIDI_FILES_DIR = pathlib.Path('data/maestro-v2.0.0')\n",
    "\n",
    "# for the others\n",
    "# Sampling rate for audio playback\n",
    "_SAMPLING_RATE = 16000\n",
    "PRINT_ITER = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mOYEoJpkKYr"
   },
   "source": [
    "용어 정리<br>real : 실제 연주자 연주<br>elec : 전자 음악 연주"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "8EZUtq-Sljod"
   },
   "outputs": [],
   "source": [
    "def midi_to_notes(midi_file: str) -> pd.DataFrame:\n",
    "  pm = pretty_midi.PrettyMIDI(midi_file)\n",
    "  instrument = pm.instruments[0]\n",
    "  notes = collections.defaultdict(list)\n",
    "\n",
    "  # Sort the notes by start time\n",
    "  sorted_notes = sorted(instrument.notes, key=lambda note: note.start)\n",
    "  prev_start = sorted_notes[0].start\n",
    "\n",
    "  for note in sorted_notes:\n",
    "    start = note.start\n",
    "    end = note.end\n",
    "    notes['pitch'].append(note.pitch)\n",
    "    notes['start'].append(start)\n",
    "    notes['end'].append(end)\n",
    "    notes['step'].append(start - prev_start)\n",
    "    notes['duration'].append(end - start)\n",
    "    prev_start = start\n",
    "\n",
    "  return pd.DataFrame({name: np.array(value) for name, value in notes.items()})\n",
    "\n",
    "def plot_piano_roll(notes: pd.DataFrame, count: Optional[int] = None):\n",
    "    if count:\n",
    "      title = f'First {count} notes'\n",
    "    else:\n",
    "      title = f'Whole track'\n",
    "      count = len(notes['pitch'])\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    plot_pitch = np.stack([notes['pitch'], notes['pitch']], axis=0)\n",
    "    plot_start_stop = np.stack([notes['start'], notes['end']], axis=0)\n",
    "    plt.plot(\n",
    "        plot_start_stop[:, :count], plot_pitch[:, :count], color=\"b\", marker=\".\")\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('Pitch')\n",
    "    _ = plt.title(title)\n",
    "\n",
    "def plot_distributions(notes: pd.DataFrame, drop_percentile=2.5):\n",
    "  plt.figure(figsize=[15, 5])\n",
    "  plt.subplot(1, 3, 1)\n",
    "  sns.histplot(notes, x=\"pitch\", bins=20)\n",
    "\n",
    "  plt.subplot(1, 3, 2)\n",
    "  max_step = np.percentile(notes['step'], 100 - drop_percentile)\n",
    "  sns.histplot(notes, x=\"step\", bins=np.linspace(0, max_step, 21))\n",
    "  \n",
    "  plt.subplot(1, 3, 3)\n",
    "  max_duration = np.percentile(notes['duration'], 100 - drop_percentile)\n",
    "  sns.histplot(notes, x=\"duration\", bins=np.linspace(0, max_duration, 21))\n",
    "\n",
    "def notes_to_midi(\n",
    "  notes: pd.DataFrame,\n",
    "  out_file: str, \n",
    "  instrument_name: str,\n",
    "  velocity: int = 100,  # note loudness\n",
    ") -> pretty_midi.PrettyMIDI:\n",
    "\n",
    "  pm = pretty_midi.PrettyMIDI()\n",
    "  instrument = pretty_midi.Instrument(\n",
    "      program=pretty_midi.instrument_name_to_program(\n",
    "          instrument_name))\n",
    "\n",
    "  prev_start = 0\n",
    "  for i, note in notes.iterrows():\n",
    "    start = float(prev_start + note['step'])\n",
    "    end = float(start + note['duration'])\n",
    "    note = pretty_midi.Note(\n",
    "        velocity=velocity,\n",
    "        pitch=int(note['pitch']),\n",
    "        start=start,\n",
    "        end=end,\n",
    "    )\n",
    "    instrument.notes.append(note)\n",
    "    prev_start = start\n",
    "\n",
    "  pm.instruments.append(instrument)\n",
    "  pm.write(out_file)\n",
    "  return pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2N7zHJrkKYs"
   },
   "source": [
    "##### 참고\n",
    "<span style='font-size:80%'>1. midi_to_notes(midi_file: str) -> pd.DataFrame<br>2. notes_to_midi(\n",
    "  notes: pd.DataFrame,\n",
    "  out_file: str, \n",
    "  instrument_name: str,\n",
    "  velocity: int = 100,  # note loudness\n",
    ") -> pretty_midi.PrettyMIDI<br>3. plot_piano_roll(notes: pd.DataFrame, count: Optional[int] = None)<br>4. def plot_distributions(notes: pd.DataFrame, drop_percentile=2.5)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8u_hAXuLkKYz",
    "outputId": "1dd455e6-bd3f-4c4c-f106-54ad38b967d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process 99.844% 2560th file process continue...\n",
      "\n",
      "End!\n",
      "total process time for 2564 files : 503.467sec\n"
     ]
    }
   ],
   "source": [
    "# data import musician AND FAKE\n",
    "# error 많이 나면 np.array 이용하기\n",
    "# filenames slicing 수 적절히 설정하기\n",
    "\n",
    "# musician_filenames = glob.glob(str(MUSICIAN_MIDI_FILES_DIR/'**/*.mid*'))\n",
    "musician_filenames = glob.glob(str(MUSICIAN_MIDI_FILES_DIR/'*.mid')) \\\n",
    "                     + glob.glob(str(MUSICIAN_MIDI_FILES_DIR/'*.MID'))\n",
    "# musician => 2700개 정도 있음\n",
    "musician_filenames = musician_filenames[:1282]\n",
    "\n",
    "elec_filenames = glob.glob(str(ELEC_MIDI_FILES_DIR/'**/*.mid*'))\n",
    "\n",
    "print(len(musician_filenames))\n",
    "print(len(elec_filenames))\n",
    "\n",
    "\n",
    "musician_all_notes = []\n",
    "elec_all_notes = []\n",
    "\n",
    "# 각 notes listz\n",
    "# 0 : musician - 사람이 연주한 데이터\n",
    "# 1 : fake - 사람이 연주하지 않은 데이터\n",
    "data_notes = [musician_all_notes, elec_all_notes]\n",
    "data_paths = [musician_filenames, elec_filenames]\n",
    "data_notes_ds = {} # tfds를 이용한 slices 이후 데이터임, (n_notes, tf_dt) tuple형의 데이터가 원소인 list\n",
    "\n",
    "train_data_notes_ds = [] # n_notes, dataset(slice)의 tuple 형태\n",
    "validation_data_notes_ds = []\n",
    "test_data_notes_ds = []\n",
    "\n",
    "data_split_ratio = {'train' : 0.8, 'validation' : 0.2, 'test' : 0.2}\n",
    "\n",
    "key_order = ['pitch', 'step', 'duration']\n",
    "not_key_order = ['start', 'end']\n",
    "\n",
    "data_type = ['musician', 'elec']\n",
    "\n",
    "# notes 수를 맞추기 위함\n",
    "n_notes = 10e9\n",
    "\n",
    "file_cnt = 0\n",
    "\n",
    "print(f'{len(musician_filenames) + len(elec_filenames)} files are ready to process')\n",
    "print('-' * PRINT_ITER)\n",
    "print()\n",
    "print('Start!')\n",
    "print()\n",
    "time.sleep(2)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i, data_path in enumerate(data_paths):\n",
    "    for f in data_path:\n",
    "        notes = midi_to_notes(f)\n",
    "        data_notes[i].append(notes)\n",
    "        if file_cnt % 10 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f'process {(file_cnt / (len(musician_filenames) + len(elec_filenames))) * 100.0:.3f}% {file_cnt}th file process continue...')\n",
    "        file_cnt += 1\n",
    "        \n",
    "    # append와 concat은 다르다!\n",
    "    data_notes[i] = pd.concat(data_notes[i])\n",
    "    data_notes[i].drop(columns=not_key_order, inplace=True)\n",
    "    n_notes = min(len(data_notes[i]), n_notes)    \n",
    "    \n",
    "for i, data_path in enumerate(data_paths):\n",
    "    _train, _test = data_notes[i][:int(n_notes * (1 - data_split_ratio['test']))], data_notes[i][int(n_notes * (1 - data_split_ratio['test'])):]\n",
    "    _train, _validation = _train[:int(len(_train) * data_split_ratio['train'])], _train[int(len(_train) * data_split_ratio['train']):]\n",
    "    n__train = len(_train)\n",
    "    n__validation = len(_validation)\n",
    "    n__test = len(_test)\n",
    "    data_dict = {'train' : (n__train, _train), 'validation' : (n__validation, _validation), 'test' : (n__test, _test)}\n",
    "    data_notes_ds[data_type[i]] = data_dict\n",
    "    \n",
    "    \n",
    "print()\n",
    "print('End!')\n",
    "\n",
    "print(f'total process time for {len(musician_filenames) + len(elec_filenames)} files : {time.time() - start_time:.3f}sec')\n",
    "# data_notes_ds Info\n",
    "# (n_data, tfds) 원소,\n",
    "# musician ==> train, validation, test\n",
    "# elec ==> train, validation, test\n",
    "# 총 6개의 원소가 들어가 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "RMnql0GoixPU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pitch       False\n",
       "step        False\n",
       "duration    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_notes_ds['musician']['train'][1].isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pitch</th>\n",
       "      <th>step</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.901686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39</td>\n",
       "      <td>0.021367</td>\n",
       "      <td>7.880319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46</td>\n",
       "      <td>0.822647</td>\n",
       "      <td>7.057671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>0.441238</td>\n",
       "      <td>6.616433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>63</td>\n",
       "      <td>0.385683</td>\n",
       "      <td>6.230751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7548</th>\n",
       "      <td>51</td>\n",
       "      <td>0.097489</td>\n",
       "      <td>0.458066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7549</th>\n",
       "      <td>55</td>\n",
       "      <td>0.094818</td>\n",
       "      <td>0.363247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7550</th>\n",
       "      <td>51</td>\n",
       "      <td>0.121528</td>\n",
       "      <td>0.241720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7551</th>\n",
       "      <td>55</td>\n",
       "      <td>0.092147</td>\n",
       "      <td>0.149572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7552</th>\n",
       "      <td>49</td>\n",
       "      <td>0.128205</td>\n",
       "      <td>0.713140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4514749 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pitch      step  duration\n",
       "0        27  0.000000  7.901686\n",
       "1        39  0.021367  7.880319\n",
       "2        46  0.822647  7.057671\n",
       "3        54  0.441238  6.616433\n",
       "4        63  0.385683  6.230751\n",
       "...     ...       ...       ...\n",
       "7548     51  0.097489  0.458066\n",
       "7549     55  0.094818  0.363247\n",
       "7550     51  0.121528  0.241720\n",
       "7551     55  0.092147  0.149572\n",
       "7552     49  0.128205  0.713140\n",
       "\n",
       "[4514749 rows x 3 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_notes_ds['musician']['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "q18an7qlE7Ne"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pitch       False\n",
       "step        False\n",
       "duration    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_notes_ds['elec']['train'][1].isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pitch</th>\n",
       "      <th>step</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.699219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.255208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49</td>\n",
       "      <td>0.016927</td>\n",
       "      <td>0.127604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61</td>\n",
       "      <td>0.200521</td>\n",
       "      <td>0.214844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>0.204427</td>\n",
       "      <td>0.203125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2431</th>\n",
       "      <td>88</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.055208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2432</th>\n",
       "      <td>91</td>\n",
       "      <td>0.073958</td>\n",
       "      <td>0.108333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2433</th>\n",
       "      <td>89</td>\n",
       "      <td>0.091667</td>\n",
       "      <td>0.061458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2434</th>\n",
       "      <td>72</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.494792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2435</th>\n",
       "      <td>88</td>\n",
       "      <td>0.040625</td>\n",
       "      <td>0.079167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4514749 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pitch      step  duration\n",
       "0        68  0.000000  0.699219\n",
       "1        64  0.583333  0.255208\n",
       "2        49  0.016927  0.127604\n",
       "3        61  0.200521  0.214844\n",
       "4        64  0.204427  0.203125\n",
       "...     ...       ...       ...\n",
       "2431     88  0.075000  0.055208\n",
       "2432     91  0.073958  0.108333\n",
       "2433     89  0.091667  0.061458\n",
       "2434     72  0.020833  0.494792\n",
       "2435     88  0.040625  0.079167\n",
       "\n",
       "[4514749 rows x 3 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_notes_ds['elec']['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4514749 entries, 0 to 2435\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Dtype  \n",
      "---  ------    -----  \n",
      " 0   pitch     int64  \n",
      " 1   step      float64\n",
      " 2   duration  float64\n",
      "dtypes: float64(2), int64(1)\n",
      "memory usage: 137.8 MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4514749 entries, 0 to 7552\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Dtype  \n",
      "---  ------    -----  \n",
      " 0   pitch     int64  \n",
      " 1   step      float64\n",
      " 2   duration  float64\n",
      "dtypes: float64(2), int64(1)\n",
      "memory usage: 137.8 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# input data nan or inf 확인\n",
    "# min, max 값 확인해서 이상치 확인하기\n",
    "\n",
    "#TODO\n",
    "# pitch도 Dtype float64로 바꿔보기 / 아니면 전부 float32로 굳이 64까지 안 써도 무방\n",
    "print(data_notes_ds['elec']['train'][1].info())\n",
    "print(data_notes_ds['musician']['train'][1].info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              pitch          step      duration\n",
      "count  4.514749e+06  4.514749e+06  4.514749e+06\n",
      "mean   6.421934e+01  9.967181e-02  1.994590e-01\n",
      "std    1.400514e+01  2.231600e-01  3.914272e-01\n",
      "min    2.100000e+01  0.000000e+00  1.041667e-03\n",
      "25%    5.500000e+01  7.812500e-03  5.078125e-02\n",
      "50%    6.400000e+01  4.791667e-02  8.854167e-02\n",
      "75%    7.400000e+01  1.223958e-01  1.916667e-01\n",
      "max    1.080000e+02  4.276562e+01  3.538932e+01\n",
      "              pitch          step      duration\n",
      "count  4.514749e+06  4.514749e+06  4.514749e+06\n",
      "mean   6.428330e+01  9.626473e-02  8.864775e-01\n",
      "std    1.460129e+01  2.316698e-01  2.671129e+00\n",
      "min    1.600000e+01  0.000000e+00  9.375000e-04\n",
      "25%    5.500000e+01  6.677344e-03  8.947641e-02\n",
      "50%    6.500000e+01  3.952980e-02  3.514947e-01\n",
      "75%    7.500000e+01  1.175210e-01  8.625000e-01\n",
      "max    1.270000e+02  4.848490e+01  1.303231e+02\n"
     ]
    }
   ],
   "source": [
    "print(data_notes_ds['elec']['train'][1].describe())\n",
    "print(data_notes_ds['musician']['train'][1].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYFg_fHlkKY2"
   },
   "source": [
    "# cycleGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# many to many에 맞게 gru 사용하기, timedistributed and reshape와 같은 함수 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EqTNbKpcogo-",
    "outputId": "b8ae311a-ded4-43a5-da8f-3f7dbe56886f"
   },
   "outputs": [],
   "source": [
    "# v2\n",
    "def get_generator(model_name, output_channels=OUTPUT_CHANNELS):\n",
    "  input_shape = (SEQ_LENGTH, output_channels)\n",
    "  # print(input_shape)\n",
    "  \n",
    "  # LSTM, Bi를 아래와 같이 단순히 쌓아도 되는지 불분명\n",
    "  # onhot encoding이나 embedding처럼 차원 조절 방법 생각\n",
    "  \n",
    "  inputs = tf.keras.Input(input_shape, name='gen_input')\n",
    "  x0 = Bidirectional(LSTM(3, return_sequences=True))(inputs)\n",
    "  x1 = Bidirectional(LSTM(6, return_sequences=True))(x0)\n",
    "  \n",
    "  x1 = Bidirectional(LSTM(12, return_sequences=True))(x1)\n",
    "  x1 = Bidirectional(LSTM(12, return_sequences=True))(x1)\n",
    "  \n",
    "  x1 = Bidirectional(LSTM(24, return_sequences=True))(x1)\n",
    "  x1 = Bidirectional(LSTM(24, return_sequences=True))(x1)\n",
    "  \n",
    "  x1 = Bidirectional(LSTM(12, return_sequences=True))(x1)\n",
    "  x1 = Bidirectional(LSTM(12, return_sequences=True))(x1)\n",
    "\n",
    "  x1 = Bidirectional(LSTM(6, return_sequences=True))(x1)\n",
    "  x2 = Bidirectional(LSTM(3, return_sequences=True))(x1)\n",
    "  \n",
    "  outputs = TimeDistributed(Dense(output_channels), name='gen_output')(x2)\n",
    "  # 마지막 layer는 timedistributed layer로! 다른 거도 가능하면 고려하기\n",
    "  model = tf.keras.Model(inputs, outputs, name=model_name)\n",
    "  return model\n",
    "\n",
    "\n",
    "# 향후 수정 후, 사용하기\n",
    "def get_discriminator(model_name, output_channels=OUTPUT_CHANNELS):\n",
    "  input_shape = (SEQ_LENGTH, output_channels)\n",
    "  # print(input_shape)\n",
    "  inputs = tf.keras.Input(input_shape, name='disc_input')\n",
    "  x0 = Bidirectional(LSTM(3, return_sequences=True))(inputs)\n",
    "  \n",
    "  x1 = Bidirectional(LSTM(6, return_sequences=True))(x0)\n",
    "  x1 = Bidirectional(LSTM(12, return_sequences=True))(x1)\n",
    "  x1 = Bidirectional(LSTM(12, return_sequences=True))(x1)\n",
    "  x1 = Bidirectional(LSTM(12, return_sequences=True))(x1)\n",
    "\n",
    "  x2 = Bidirectional(LSTM(12, return_sequences=True))(x1)\n",
    "  \n",
    "  # activation='sigmoid'를 loss nan 문제 해결방법으로 사용해보기\n",
    "  outputs = TimeDistributed(Dense(24), name='disc_output')(x2)\n",
    "  \n",
    "  # 마지막 layer는 timedistributed layer로! 다른 거도 가능하면 고려하기\n",
    "  model = tf.keras.Model(inputs, outputs, name=model_name)\n",
    "  return model\n",
    "    \n",
    "\n",
    "# 향후 종류를 다양하게 변형하기 / 여기에서 pretrained model 이용함\n",
    "# gen_musician = tf.keras.models.load_model('model_save/v17-2/gen_elec-epoch2.h5')\n",
    "# gen_elec = tf.keras.models.load_model('model_save/v17-2/gen_elec-epoch2.h5')\n",
    "gen_musician = get_generator('gen_musician')\n",
    "gen_elec = get_generator('gen_elec')\n",
    "\n",
    "disc_musician = get_discriminator('disc_musician')\n",
    "disc_elec = get_discriminator('disc_elec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, BatchNormalization, \\\n",
    "    MaxPool2D, GlobalMaxPool2D\n",
    "def build_convnet(shape=(112, 112, 3)):\n",
    "    momentum = .9\n",
    "    model = keras.Sequential()\n",
    "    model.add(Conv2D(64, (3,3), input_shape=shape,\n",
    "        padding='same', activation='relu'))\n",
    "    model.add(Conv2D(64, (3,3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=momentum))\n",
    "    \n",
    "    model.add(MaxPool2D())\n",
    "    \n",
    "    model.add(Conv2D(128, (3,3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(128, (3,3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=momentum))\n",
    "    \n",
    "    model.add(MaxPool2D())\n",
    "    \n",
    "    model.add(Conv2D(256, (3,3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(256, (3,3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=momentum))\n",
    "    \n",
    "    model.add(MaxPool2D())\n",
    "    \n",
    "    model.add(Conv2D(512, (3,3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(512, (3,3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization(momentum=momentum))\n",
    "    \n",
    "    # flatten...\n",
    "    model.add(GlobalMaxPool2D())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_8 (Conv2D)           (None, 112, 112, 64)      1792      \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 112, 112, 64)      36928     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 112, 112, 64)     256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 56, 56, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 56, 56, 128)       73856     \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 56, 56, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 56, 56, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 28, 28, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 28, 28, 256)       295168    \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 28, 28, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 28, 28, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 14, 14, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 14, 14, 512)       1180160   \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 14, 14, 512)      2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " global_max_pooling2d_1 (Glo  (None, 512)              0         \n",
      " balMaxPooling2D)                                                \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,689,216\n",
      "Trainable params: 4,687,296\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "b = build_convnet()\n",
    "b.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed, GRU, Dense, Dropout\n",
    "def action_model(shape=(5, 112, 112, 3), nbout=3):\n",
    "    # Create our convnet with (112, 112, 3) input shape\n",
    "    convnet = build_convnet(shape[1:])\n",
    "    \n",
    "    # then create our final model\n",
    "    model = keras.Sequential()\n",
    "    # add the convnet with (5, 112, 112, 3) shape\n",
    "    model.add(TimeDistributed(convnet, input_shape=shape))\n",
    "    # here, you can also use GRU or LSTM\n",
    "    model.add(GRU(64))\n",
    "    # and finally, we make a decision network\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(nbout, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = action_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed (TimeDistr  (None, 5, 512)           4689216   \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 64)                110976    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              66560     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               65664     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,465,667\n",
      "Trainable params: 5,463,747\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "a.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mh_g0wKQTEup",
    "outputId": "a350306c-6c0c-4374-e225-23b78751b20e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"gen_musician\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gen_input (InputLayer)      [(None, 1024, 3)]         0         \n",
      "                                                                 \n",
      " bidirectional_82 (Bidirecti  (None, 1024, 6)          168       \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_83 (Bidirecti  (None, 1024, 12)         624       \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_84 (Bidirecti  (None, 1024, 24)         2400      \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_85 (Bidirecti  (None, 1024, 24)         3552      \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_86 (Bidirecti  (None, 1024, 48)         9408      \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_87 (Bidirecti  (None, 1024, 48)         14016     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_88 (Bidirecti  (None, 1024, 24)         5856      \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_89 (Bidirecti  (None, 1024, 24)         3552      \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_90 (Bidirecti  (None, 1024, 12)         1488      \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_91 (Bidirecti  (None, 1024, 6)          384       \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " gen_output (TimeDistributed  (None, 1024, 3)          21        \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 41,469\n",
      "Trainable params: 41,469\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gen_musician.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"disc_musician\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " disc_input (InputLayer)     [(None, 1024, 3)]         0         \n",
      "                                                                 \n",
      " bidirectional_102 (Bidirect  (None, 1024, 6)          168       \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " bidirectional_103 (Bidirect  (None, 1024, 12)         624       \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " bidirectional_104 (Bidirect  (None, 1024, 24)         2400      \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " bidirectional_105 (Bidirect  (None, 1024, 24)         3552      \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " bidirectional_106 (Bidirect  (None, 1024, 24)         3552      \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " bidirectional_107 (Bidirect  (None, 1024, 24)         3552      \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " disc_output (TimeDistribute  (None, 1024, 24)         600       \n",
      " d)                                                              \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,448\n",
      "Trainable params: 14,448\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "disc_musician.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "18kz0PBwkKY3"
   },
   "outputs": [],
   "source": [
    "# loss function\n",
    "\n",
    "loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "# losses_ratio = {'disc_loss' : 1.0, 'gen_loss' : 1.0, 'cycle_loss' : 1.0}\n",
    "\n",
    "for_nan_checking = {}\n",
    "for_prev_nan_checking = {}\n",
    "\n",
    "# first_one_checked = False\n",
    "nan_checked = False\n",
    "\n",
    "def disc_loss(real, generated):\n",
    "#     real += tf.convert_to_tensor(1)\n",
    "#     real = tf.math.scalar_mul(0.5, real)\n",
    "    \n",
    "#     generated += tf.convert_to_tensor(1)\n",
    "#     generated = tf.math.scalar_mul(0.5, real)\n",
    "    global for_nan_checking, for_prev_nan_checking, nan_checked\n",
    "    \n",
    "    real_loss = loss_obj(tf.ones_like(real), real)\n",
    "    if real_loss != real_loss:\n",
    "        for_nan_checking['real_loss'] = real\n",
    "        nan_checked = True\n",
    "    \n",
    "    if not nan_checked:\n",
    "        for_prev_nan_checking['real_loss'] = real\n",
    "        \n",
    "    \n",
    "    fake_loss = loss_obj(tf.zeros_like(generated), generated)\n",
    "    if fake_loss != fake_loss:\n",
    "        for_nan_checking['fake_loss'] = generated\n",
    "        nan_checked = True\n",
    "        \n",
    "    if not nan_checked:\n",
    "        for_prev_nan_checking['fake_loss'] = generated\n",
    "    \n",
    "    total_loss = real_loss + fake_loss\n",
    "    \n",
    "    \n",
    "    return total_loss * 0.5\n",
    "\n",
    "def gen_loss(generated):\n",
    "#     generated += tf.convert_to_tensor(1)\n",
    "#     generated = tf.math.scalar_mul(0.5, generated)\n",
    "    global for_nan_checking, for_prev_nan_checking, nan_checked\n",
    "\n",
    "    t_gen_loss = loss_obj(tf.ones_like(generated), generated)\n",
    "    \n",
    "    if t_gen_loss != t_gen_loss:\n",
    "        for_nan_checking['gen_loss'] = generated\n",
    "        nan_checked = True\n",
    "        \n",
    "    if not nan_checked:\n",
    "        for_prev_nan_checking['gen_loss'] = generated\n",
    "        \n",
    "    return t_gen_loss * 1\n",
    "\n",
    "def cycle_loss(real_data, cycled_data):\n",
    "    global for_nan_checking, for_prev_nan_checking, nan_checked\n",
    "    \n",
    "    t_cycle_loss = tf.reduce_mean(tf.abs(real_data - cycled_data))\n",
    "    \n",
    "    if t_cycle_loss != t_cycle_loss:\n",
    "        for_nan_checking['cycle_loss'] = (real_data, cycled_data)\n",
    "        nan_checked = True\n",
    "        \n",
    "    if not nan_checked:\n",
    "        for_prev_nan_checking['cycle_loss'] = (real_data, cycled_data)\n",
    "        \n",
    "    return t_cycle_loss * 10\n",
    "\n",
    "def iden_loss(real_data, gen_from_real_data):\n",
    "    global for_nan_checking, for_prev_nan_checking, nan_checked\n",
    "    \n",
    "    t_iden_loss = tf.reduce_mean(tf.abs(real_data - gen_from_real_data))\n",
    "    \n",
    "    if t_iden_loss != t_iden_loss:\n",
    "        for_nan_checking['iden_loss'] = (real_data, gen_from_real_data)\n",
    "        nan_checked = True\n",
    "        \n",
    "    if not nan_checked:\n",
    "        for_prev_nan_checking['iden_loss'] = (real_data, gen_from_real_data)\n",
    "    \n",
    "    return t_iden_loss * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "XZent3AukKY3"
   },
   "outputs": [],
   "source": [
    "# optimizer\n",
    "\n",
    "# LR_gen = 9e-4\n",
    "# LR_disc = 1e-6\n",
    "# B1 = 0.5 \n",
    "\n",
    "gen_musician_opt = tf.keras.optimizers.Adam(learning_rate=LR_gen)\n",
    "gen_elec_opt = tf.keras.optimizers.Adam(learning_rate=LR_gen)\n",
    "\n",
    "disc_musician_opt = tf.keras.optimizers.Adam(learning_rate=LR_disc)\n",
    "disc_elec_opt = tf.keras.optimizers.Adam(learning_rate=LR_disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "D7nFJI6xkKY3"
   },
   "outputs": [],
   "source": [
    "# # checkpoint_path의 경우 계속해서 바꿔줘야 에러가 나지 않음, 이 cell(+위에 model 정의 cell)을 반복해서 실행하면 error 발생함\n",
    "# checkpoint_path = \"./checkpoint/train_2022_11_10\"\n",
    "\n",
    "# ckpt = tf.train.Checkpoint(gen_musician=gen_musician,\n",
    "#                            gen_elec=gen_elec,\n",
    "#                            disc_musician=disc_musician,\n",
    "#                            disc_elec=disc_elec,\n",
    "#                            gen_musician_opt=gen_musician_opt,\n",
    "#                            gen_elec_opt=gen_elec_opt,\n",
    "#                            disc_musician_opt=disc_musician_opt,\n",
    "#                            disc_elec_opt=disc_elec_opt)\n",
    "\n",
    "# ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# # if a checkpoint exists, restore the latest checkpoint.\n",
    "# if ckpt_manager.latest_checkpoint:\n",
    "#   ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "#   print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "bfagpkxhkKY3"
   },
   "outputs": [],
   "source": [
    "# real_x : musician data\n",
    "# real_y : elec data\n",
    "# g : gen elec\n",
    "# f : gen music\n",
    "\n",
    "@tf.function\n",
    "def train_step(real_x, real_y, losses_ratio):\n",
    "    # persistent is set to True because the tape is used more than\n",
    "    # once to calculate the gradients.\n",
    "    all_loss = []\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # Generator G translates X -> Y\n",
    "        # Generator F translates Y -> X.\n",
    "        fake_y = gen_elec(real_x, training=True)\n",
    "        cycled_x = gen_musician(fake_y, training=True)\n",
    "\n",
    "        fake_x = gen_musician(real_y, training=True)\n",
    "        cycled_y = gen_elec(fake_x, training=True)\n",
    "\n",
    "        # same_x and same_y are used for identity loss.\n",
    "        same_x = gen_musician(real_x, training=True)\n",
    "        same_y = gen_elec(real_y, training=True)\n",
    "        \n",
    "        real_x_pitch = np.round(real_x.numpy()[:, :, 0] * 128)\n",
    "        fake_y_pitch = np.round(fake_y.numpy()[:, :, 0] * 128)\n",
    "        \n",
    "        real_y_pitch = np.round(real_y.numpy()[:, :, 0] * 128)\n",
    "        fake_x_pitch = np.round(fake_x.numpy()[:, :, 0] * 128)\n",
    "        \n",
    "        # print(real_x_pitch[0], fake_y_pitch[0])\n",
    "        \n",
    "#         print(type((real_x_pitch == fake_y_pitch)))\n",
    "        false_pitch_sum = np.sum(np.abs(real_x_pitch - fake_y_pitch)) + np.sum(np.abs(real_y_pitch - fake_x_pitch))\n",
    "        \n",
    "        false_pitch_sum = np.sqrt(false_pitch_sum).item()\n",
    "        \n",
    "        # 모든 pitch가 같아, false_pitch_sum == 0인 경우 방지 \n",
    "        false_pitch_sum = false_pitch_sum if false_pitch_sum > 1.0 else 1\n",
    "        \n",
    "#         print(false_pitch_sum)\n",
    "        \n",
    "        \n",
    "#         print(real_x_pitch)\n",
    "        \n",
    "        # print(real_x_pitch == fake_y_pitch)\n",
    "        \n",
    "        \n",
    "#         print(real_x[0].numpy()[:, 0] == same_x[0].numpy()[:, 0])\n",
    "#         print(type(same_x))\n",
    "#         print(same_x.numpy())\n",
    "        \n",
    "\n",
    "        disc_real_x = disc_musician(real_x, training=True)\n",
    "        disc_real_y = disc_elec(real_y, training=True)\n",
    "\n",
    "        disc_fake_x = disc_musician(fake_x, training=True)\n",
    "        disc_fake_y = disc_elec(fake_y, training=True)\n",
    "\n",
    "        # calculate the loss\n",
    "        gen_elec_loss = gen_loss(disc_fake_y) #* false_pitch_sum\n",
    "        gen_musician_loss = gen_loss(disc_fake_x) #* false_pitch_sum\n",
    "        total_cycle_loss = cycle_loss(real_x, cycled_x) + cycle_loss(real_y, cycled_y)\n",
    "\n",
    "        elec_iden_loss = iden_loss(real_y, same_y)\n",
    "        musician_iden_loss = iden_loss(real_x, same_x)\n",
    "\n",
    "        # Total generator loss = adversarial loss + cycle loss\n",
    "        total_gen_elec_loss = gen_elec_loss*losses_ratio['gen'] + total_cycle_loss*losses_ratio['cyc'] + elec_iden_loss*losses_ratio['iden']\n",
    "        total_gen_musician_loss = gen_musician_loss*losses_ratio['gen'] + total_cycle_loss*losses_ratio['cyc'] + musician_iden_loss*losses_ratio['iden']\n",
    "\n",
    "        disc_musician_loss = disc_loss(disc_real_x, disc_fake_x)*losses_ratio['disc']\n",
    "        disc_elec_loss = disc_loss(disc_real_y, disc_fake_y)*losses_ratio['disc']\n",
    "\n",
    "        all_loss = [tf.identity(gen_elec_loss), \n",
    "                    tf.identity(gen_musician_loss), \n",
    "                    tf.identity(total_cycle_loss), \n",
    "                    tf.identity(total_gen_elec_loss),\n",
    "                    tf.identity(total_gen_musician_loss),\n",
    "                    tf.identity(disc_musician_loss),\n",
    "                    tf.identity(disc_elec_loss),\n",
    "                    tf.identity(musician_iden_loss),\n",
    "                    tf.identity(elec_iden_loss)]\n",
    "\n",
    "\n",
    "    # Calculate the gradients for generator and discriminator\n",
    "    gen_elec_gradients = tape.gradient(total_gen_elec_loss, \n",
    "                                          gen_elec.trainable_variables)\n",
    "    \n",
    "    gen_musician_gradients = tape.gradient(total_gen_musician_loss, \n",
    "                                          gen_musician.trainable_variables)\n",
    "    \n",
    "    disc_musician_gradients = tape.gradient(disc_musician_loss, \n",
    "                                              disc_musician.trainable_variables)\n",
    "    disc_elec_gradients = tape.gradient(disc_elec_loss, \n",
    "                                              disc_elec.trainable_variables)\n",
    "\n",
    "    # Apply the gradients to the optimizer\n",
    "    gen_elec_opt.apply_gradients(zip(gen_elec_gradients, \n",
    "                                              gen_elec.trainable_variables))\n",
    "\n",
    "    gen_musician_opt.apply_gradients(zip(gen_musician_gradients, \n",
    "                                              gen_musician.trainable_variables))\n",
    "    \n",
    "    disc_musician_opt.apply_gradients(zip(disc_musician_gradients,\n",
    "                                                  disc_musician.trainable_variables))\n",
    "    \n",
    "    disc_elec_opt.apply_gradients(zip(disc_elec_gradients,\n",
    "                                                  disc_elec.trainable_variables))\n",
    "    \n",
    "    return all_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gPFvnti5R31u",
    "outputId": "c7a27ce4-baa1-41a4-b6f4-b2efdc1a0523"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_train_notes shape (4408, 1024, 3)\n",
      "\n",
      "notes_ds element spec TensorSpec(shape=(1024, 3), dtype=tf.float64, name=None)\n",
      "\n",
      "train_ds spec TensorSpec(shape=(None, 1024, 3), dtype=tf.float32, name=None)\n",
      "\n",
      "--------------------------------------------------\n",
      "seq_train_notes shape (4408, 1024, 3)\n",
      "\n",
      "notes_ds element spec TensorSpec(shape=(1024, 3), dtype=tf.float64, name=None)\n",
      "\n",
      "train_ds spec TensorSpec(shape=(None, 1024, 3), dtype=tf.float32, name=None)\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def notes_to_trainable_dataset(all_notes, n_data):\n",
    "    # 나중에 없애도 무방\n",
    "    \n",
    "    key_order = ['pitch', 'step', 'duration']\n",
    "    train_notes = np.stack([all_notes[key] for key in key_order], axis=1)\n",
    "    n_notes, n_features = train_notes.shape \n",
    "\n",
    "    train_notes = train_notes[:n_notes - (n_notes % SEQ_LENGTH)]\n",
    "    train_notes[:, 0] = train_notes[:, 0] / VOCAB_SIZE\n",
    "\n",
    "    seq_train_notes = train_notes.reshape(-1, SEQ_LENGTH, 3)\n",
    "    # seq_train_notes = seq_train_notes / np.array([VOCAB_SIZE, 0.0, 0.0])\n",
    "\n",
    "    print(f'seq_train_notes shape {seq_train_notes.shape}')\n",
    "    print()\n",
    "\n",
    "    # 여기를 다르게 바꿈 / pretrain에서 사용한 data를 사용하지 않기 위해서 :n_data -> n_data:로\n",
    "    seq_train_notes = seq_train_notes[:n_data]\n",
    "    \n",
    "    seq_ds = tf.data.Dataset.from_tensor_slices(seq_train_notes)\n",
    "    print(f'notes_ds element spec {seq_ds.element_spec}')\n",
    "    print()\n",
    "\n",
    "    # BUFFER_SIZE = n_notes - SEQ_LENGTH\n",
    "    train_ds = seq_ds.map(lambda x: (tf.cast(x, tf.float32)), num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(BATCH_SIZE, num_parallel_calls=tf.data.experimental.AUTOTUNE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    print(f'train_ds spec {train_ds.element_spec}')\n",
    "    print()\n",
    "\n",
    "    print('-' * PRINT_ITER)\n",
    "\n",
    "    return train_ds\n",
    "\n",
    "\n",
    "musician_train_ds = notes_to_trainable_dataset(data_notes_ds['musician']['train'][1], 300)\n",
    "\n",
    "elec_train_ds = notes_to_trainable_dataset(data_notes_ds['elec']['train'][1], 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "8hTlN9117HLi"
   },
   "outputs": [],
   "source": [
    "# @tf.function을 이용하여 code를 실행하는 도중에 .nmupy()와 같은 code 실행 가능\n",
    "# https://github.com/tgjeon/TF-Eager-Execution-Guide-KR/blob/master/guide.md 참고\n",
    "# https://www.tensorflow.org/api_docs/python/tf/config/run_functions_eagerly\n",
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "k4RFr6e08mME"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 36\n",
    "VERSION = '19'\n",
    "SAVE_PATH = 'runs_test5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "VhRzDcg52pmu"
   },
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter(f'{SAVE_PATH}/v{VERSION}-epoch{EPOCHS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJHV5R7OKf8j"
   },
   "source": [
    "# problem\n",
    "1. nan 값 -> https://data-newbie.tistory.com/281\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_cnt = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. learning rate가 커서 gradient discent 과정에서 튕겨져 나감\n",
    "2. learning rate의 크기 차이로 gen과 disc의 학습에 충분한 시간이 부여되지 않음\n",
    "\n",
    "-> learning rate에 대해서 gen, disc 각각 작게, 크게 했을때 변화하는 값 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lc1xCXbDkKY3",
    "outputId": "b8c3b1ac-4aaa-41ff-ffd7-9e9b98204fd1",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6011 (pid 2023924), started 0:00:39 ago. (Use '!kill 2023924' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-34d8d9b17435c201\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-34d8d9b17435c201\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6011;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    }
   ],
   "source": [
    "# tensorboard graph color rgb : #1fddff\n",
    "# disc -> 틀렸을때, 강한 피드백(scalar)을 작용 -> 학습에 오랜 시간\n",
    "losses_ratio = {'disc' : 1.0, 'gen' : 1.0, 'cyc' : 1.0, 'iden' : 1.0} # cycle, iden loss ratio는 서로 종속적임(in tf document)\n",
    "all_loss = []\n",
    "all_loss_name = ['gen_elec_loss', \n",
    "                 'gen_musician_loss',\n",
    "                 'total_cycle_loss', \n",
    "                 'total_gen_elec_loss',\n",
    "                 'total_gen_musician_loss',\n",
    "                 'disc_musician_loss',\n",
    "                 'disc_elec_loss',\n",
    "                 'musician_iden_loss',\n",
    "                 'elec_iden_loss']\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {SAVE_PATH} --host 0.0.0.0 --port=6011\n",
    "\n",
    "# 단순히 모델의 가중치 업데이트 횟수를 의미함\n",
    "# 향후 이 값을 바탕으로 epoch를 계산 가능 | \n",
    "# update_n = epoch * (n_notes / (seq_length * batch_size)) = epoch * (n_seq_notes / batch_size)\n",
    "\n",
    "# update_cnt = 0\n",
    "\n",
    "total_start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    n = 0\n",
    "    \n",
    "    for musician, elec in tf.data.Dataset.zip((musician_train_ds, elec_train_ds)):\n",
    "        # musician = tf.cast(musician, dtype=tf.float32)\n",
    "        # elec = tf.cast(elec, dtype=tf.float32)\n",
    "                \n",
    "        all_loss = [0 for _ in range(len(all_loss_name))]   \n",
    "\n",
    "        for_all_loss = train_step(musician, elec, losses_ratio)\n",
    "        for i, loss in enumerate(for_all_loss):\n",
    "            all_loss[i] += loss.numpy().item()\n",
    "\n",
    "        if update_cnt % 100 == 0:\n",
    "            print('-', end='')\n",
    "#         if update_cnt % 4500 == 0 and update_cnt != 0:\n",
    "#             gen_musician.save(f'model_save/v{VERSION}/gen_musician-epoch{EPOCHS}-step4500.h5')\n",
    "#             gen_elec.save(f'model_save/v{VERSION}/gen_elec-epoch{EPOCHS}-step4500.h5')\n",
    "#             disc_musician.save(f'model_save/v{VERSION}/disc_musician-epoch{EPOCHS}-step4500.h5')\n",
    "#             disc_elec.save(f'model_save/v{VERSION}/disc_elec-epoch{EPOCHS}.h5')\n",
    "        \n",
    "        n += 1\n",
    "        update_cnt += 1\n",
    "\n",
    "        # 아래 부분에서 BATCH_SIZE로 나눌 지에 관해 고민 필요\n",
    "        all_loss = [loss for loss in all_loss]\n",
    "        for loss, loss_name in zip(all_loss, all_loss_name):\n",
    "            writer.add_scalars('check_info/', {loss_name : loss}, update_cnt)\n",
    "\n",
    "    # clear_output(wait=True)\n",
    "    \n",
    "    # note를 시각화할 수 있는 거 넣기\n",
    "    # ex) generate_images(generator_g, sample_horse)\n",
    "    \n",
    "#     if (epoch + 1) % 3 == 0:\n",
    "#         ckpt_save_path = ckpt_manager.save()\n",
    "#         print(f'Saving checkpoint for each {epoch + 1} at {ckpt_save_path}')\n",
    "    \n",
    "    print(f'Time taken for epoch {epoch + 1} is {time.time() - start:.3f} sec\\n')\n",
    "\n",
    "print()\n",
    "print('-' * PRINT_ITER)\n",
    "print(f'Time taken for total {EPOCHS} epoch : {time.time() - total_start_time:.3f} sec')\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Za3s9whV6pH"
   },
   "outputs": [],
   "source": [
    "def generate_domain_notes(input_ds, model):\n",
    "    result_list = []\n",
    "    generated_notes_list = []\n",
    "    n_batch_data = tf.data.experimental.cardinality(input_ds).numpy().item()\n",
    "    # n_batch_data = 1\n",
    "\n",
    "    for inp in input_ds.take(n_batch_data):\n",
    "        gen_music = model(inp)\n",
    "        result_list.append(gen_music)\n",
    "\n",
    "    for result in result_list:\n",
    "        pitchs = result[:, :, 0]\n",
    "        steps = result[:, :, 1]\n",
    "        durations = result[:, :, 2]\n",
    "\n",
    "        pitchs = tf.reshape(pitchs, [-1])\n",
    "        pitchs = tf.math.scalar_mul(VOCAB_SIZE, pitchs)\n",
    "        pitchs = tf.math.round(pitchs)\n",
    "        pitchs = tf.cast(pitchs, dtype=tf.int32)\n",
    "        pitchs = pitchs.numpy()\n",
    "\n",
    "        durations = tf.reshape(durations, [-1])\n",
    "        durations = durations.numpy()\n",
    "\n",
    "        steps = tf.reshape(steps, [-1])\n",
    "        steps = steps.numpy() \n",
    "\n",
    "        generated_notes = []\n",
    "        prev_start = 0\n",
    "\n",
    "        for pitch, duration, step in zip(pitchs, durations, steps):\n",
    "            pitch, duration, step = pitch.item(), duration.item(), step.item()\n",
    "\n",
    "            if pitch > VOCAB_SIZE - 1:\n",
    "                pitch = VOCAB_SIZE - 1\n",
    "            if pitch < 0:\n",
    "                pitch = 0\n",
    "                \n",
    "            start = prev_start + step \n",
    "            end = start + duration\n",
    "            input_note = (pitch, step, duration)\n",
    "            generated_notes.append((*input_note, start, end))\n",
    "\n",
    "            prev_start = start\n",
    "\n",
    "        generated_notes = pd.DataFrame(\n",
    "            generated_notes, columns=(*key_order, 'start', 'end')\n",
    "        )\n",
    "\n",
    "        generated_notes_list.append(generated_notes)\n",
    "\n",
    "    generated_notes_all = pd.concat(generated_notes_list, ignore_index=True)\n",
    "    return generated_notes_all\n",
    "\n",
    "def notes_to_testable_dataset(all_notes):\n",
    "    key_order = ['pitch', 'step', 'duration']\n",
    "    test_notes = np.stack([all_notes[key] for key in key_order], axis=1)\n",
    "    n_notes, n_features = test_notes.shape \n",
    "    \n",
    "    n_notes_for_seq = n_notes - (n_notes % SEQ_LENGTH)\n",
    "    test_notes = test_notes[:n_notes_for_seq]\n",
    "    \n",
    "    test_notes = test_notes.reshape(-1, SEQ_LENGTH, 3)\n",
    "    print(f'test_notes shape {test_notes.shape}')\n",
    "    print()\n",
    "    \n",
    "    notes_ds = tf.data.Dataset.from_tensor_slices(test_notes)\n",
    "    print(f'notes_ds element spec {notes_ds.element_spec}')\n",
    "    print()\n",
    "\n",
    "    seq_ds = notes_ds\n",
    "    print(f'seq_ds element_spec {seq_ds.element_spec}')\n",
    "    print()\n",
    "\n",
    "    # BUFFER_SIZE = n_notes - SEQ_LENGTH\n",
    "    test_ds = seq_ds.batch(n_notes_for_seq, num_parallel_calls=tf.data.experimental.AUTOTUNE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    print(f'test_ds spec {test_ds.element_spec}')\n",
    "    print()\n",
    "\n",
    "    print('-' * PRINT_ITER)\n",
    "\n",
    "    return test_ds\n",
    "\n",
    "def generate_notes(midi_filepath, model):\n",
    "    notes = midi_to_notes(midi_filepath)\n",
    "    test_ds = notes_to_testable_dataset(notes)\n",
    "    generated_notes = generate_domain_notes(test_ds, model)\n",
    "    return generated_notes\n",
    "    \n",
    "\n",
    "midi_filepath = elec_filenames[100]\n",
    "\n",
    "generated_notes = generate_notes(midi_filepath, gen_musician)\n",
    "print(generated_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8T8_ab8N3WVG"
   },
   "outputs": [],
   "source": [
    "midi_filepath = elec_filenames[666]\n",
    "\n",
    "generated_notes = generate_notes(midi_filepath, gen_musician)\n",
    "generated_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z-ejEvRk3WVH"
   },
   "outputs": [],
   "source": [
    "real_notes = midi_to_notes(midi_filepath)\n",
    "real_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sqrt(np.sum(np.abs((generated_notes['pitch'] - real_notes['pitch'][len(generated_notes)]).to_numpy())) / (len(generated_notes) / (SEQ_LENGTH*BATCH_SIZE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = f'model_save/v{VERSION}/test.mid'\n",
    "out_pm = notes_to_midi(\n",
    "    generated_notes, out_file=test_file, instrument_name=instrument_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6a8Z-6MOTBCb"
   },
   "outputs": [],
   "source": [
    "out_file = f'model_save/v{VERSION}/output_gen_musician-epoch{EPOCHS}.mid'\n",
    "out_pm = notes_to_midi(\n",
    "    generated_notes, out_file=out_file, instrument_name=instrument_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i6mAc0ui3WVH"
   },
   "outputs": [],
   "source": [
    "out_file = f'model_save/v{VERSION}/output_real_elec-epoch{EPOCHS}.mid'\n",
    "out_pm = notes_to_midi(\n",
    "    real_notes, out_file=out_file, instrument_name=instrument_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_musician.save(f'model_save/v{VERSION}/gen_musician-epoch{EPOCHS}.h5')\n",
    "gen_elec.save(f'model_save/v{VERSION}/gen_elec-epoch{EPOCHS}.h5')\n",
    "disc_musician.save(f'model_save/v{VERSION}/disc_musician-epoch{EPOCHS}.h5')\n",
    "disc_elec.save(f'model_save/v{VERSION}/disc_elec-epoch{EPOCHS}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "VE5Zd7tzTHun"
   },
   "outputs": [],
   "source": [
    "# # generator 불러오기\n",
    "# gen_musician_ckpt_path = ''\n",
    "# gen_musician.load_weights(gen_musician_ckpt_path)\n",
    "\n",
    "# gen_elec_ckpt_path = '' \n",
    "# gen_elec.load_weights(gen_elec_ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sctRLZFeDheR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MuaIw8UXDheS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "76d2a536684e05cb77c92a45f34021a8fe6e78c156cd0f72220424a41f6bdcdc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
